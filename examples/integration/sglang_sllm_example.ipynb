{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc205f35",
   "metadata": {},
   "source": [
    "# SGLang + ServerlessLLM Integration Example\n",
    "\n",
    "This notebook demonstrates the integration between SGLang and ServerlessLLM for optimized model loading and inference.\n",
    "\n",
    "We will:\n",
    "1. Install dependencies.\n",
    "2. Convert a HuggingFace model (`Qwen/Qwen3-0.6B`) to ServerlessLLM format.\n",
    "3. Run a benchmark comparing the loading time of the ServerlessLLM format vs. the standard format.\n",
    "\n",
    "**Note:** We use `uv` for faster installation as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a98408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ uv is already installed.\n",
      "‚úÖ ServerlessLLM is already installed.\n",
      "\n",
      "üéâ All done! ServerlessLLM is ready to use.\n",
      "   Built from: /scratch/users/ntu/ktang022/ServerlessLLM/sllm\n",
      "   Built from: /scratch/users/ntu/ktang022/ServerlessLLM/sllm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Install uv\n",
    "if shutil.which(\"uv\"):\n",
    "    print(\"‚úÖ uv is already installed.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Installing uv...\")\n",
    "    !pip install uv\n",
    "    print(\"‚úÖ uv installed.\")\n",
    "\n",
    "# Install ServerlessLLM\n",
    "if shutil.which(\"sllm\") and shutil.which(\"sllm-store\"):\n",
    "    print(\"‚úÖ ServerlessLLM is already installed.\")\n",
    "else:\n",
    "    print(\"üì• Cloning ServerlessLLM repository...\")\n",
    "    !git clone https://github.com/ServerlessLLM/ServerlessLLM.git\n",
    "    print(\"‚úÖ Repository cloned.\")\n",
    "    \n",
    "    # Install sllm_store\n",
    "    if shutil.which(\"sllm-store\"):\n",
    "        print(\"‚úÖ ServerlessLLM installed.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Installing sllm_store...\")\n",
    "        !uv pip install -e ServerlessLLM/sllm_store\n",
    "        print(\"‚úÖ sllm_store installed.\")\n",
    "    \n",
    "    # Install ServerlessLLM\n",
    "    if shutil.which(\"sllm\") or shutil.which(\"serverless-llm\"):\n",
    "        print(\"‚úÖ ServerlessLLM is already installed.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Installing ServerlessLLM...\")\n",
    "        !uv pip install -e ServerlessLLM\n",
    "        print(\"‚úÖ ServerlessLLM installed.\")\n",
    "\n",
    "print(\"\\nüéâ All done! ServerlessLLM is ready to use.\")\n",
    "import sllm,sllm_store\n",
    "sllm_location = os.path.dirname(sllm.__file__)\n",
    "print(f\"   Built from: {sllm_location}\")\n",
    "\n",
    "sllm_store_location = os.path.dirname(sllm_store.__file__)\n",
    "print(f\"   Built from: {sllm_location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77e53e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sglang is already installed.\n",
      "   Built from: /scratch/users/ntu/ktang022/sglang\n"
     ]
    }
   ],
   "source": [
    "# Install SGLang with ServerlessLLM support from the specific feature branch\n",
    "if shutil.which(\"sglang\"):\n",
    "    print(\"‚úÖ sglang is already installed.\")\n",
    "else:\n",
    "    print(\"Installing sglang\")\n",
    "    !git clone -b feat-sllm-load-format https://github.com/JustinTong0323/sglang \n",
    "    \n",
    "    !cd sglang_fork/python && uv pip install -e .\n",
    "    print(\"‚úÖ sglang is installed.\")\n",
    "\n",
    "import sglang\n",
    "import os\n",
    "if hasattr(sglang, '__file__') and sglang.__file__ is not None:\n",
    "    sglang_location = os.path.dirname(sglang.__file__)\n",
    "    print(f\"   Built from: {sglang_location}\")\n",
    "else:\n",
    "    # Try to get location from __path__ for namespace packages\n",
    "    if hasattr(sglang, '__path__'):\n",
    "        sglang_location = str(sglang.__path__[0]) if sglang.__path__ else \"unknown\"\n",
    "        print(f\"   Built from: {sglang_location}\")\n",
    "    else:\n",
    "        print(f\"   Built from: unknown (namespace package)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bf979",
   "metadata": {},
   "source": [
    "## 2. Convert Model to ServerlessLLM Format\n",
    "\n",
    "We use the `Qwen/Qwen3-0.6B` model for this demonstration. \n",
    "The conversion script optimizes the model checkpoints for fast loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e664933-af06-4a69-8ae6-35fda2df938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/users/ntu/ktang022/scratch/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ce99b6-90e4-48e0-bd32-7b29bafd859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "sllm_storage_path = \"/home/users/ntu/ktang022/scratch/models\"\n",
    "\n",
    "hf_storage_path = \"/home/users/ntu/ktang022/scratch/hf_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a76f5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]\n",
      "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
      "\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [00:00<00:00, 5.56MB/s]\u001b[A\u001b[A\n",
      "Fetching 7 files:  14%|‚ñà‚ñà‚ñà‚ñä                       | 1/7 [00:00<00:02,  2.48it/s]\n",
      "\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 239/239 [00:00<00:00, 2.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer_config.json: 9.73kB [00:00, 25.0MB/s]A\n",
      "\n",
      "\n",
      "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
      "merges.txt: 1.67MB [00:00, 12.4MB/s][A\n",
      "Fetching 7 files:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 3/7 [00:00<00:00,  6.58it/s]\n",
      "\n",
      "vocab.json: 607kB [00:00, 5.41MB/s]\u001b[A\u001b[A\n",
      "tokenizer.json:   0%|                               | 0.00/11.4M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "vocab.json: 2.78MB [00:00, 14.4MB/s]                | 0.00/1.50G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4M/11.4M [00:01<00:00, 7.08MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|‚ñè                  | 18.8M/1.50G [00:02<03:04, 8.06MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   1%|‚ñé                  | 21.1M/1.50G [00:02<03:04, 8.04MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   2%|‚ñé                  | 28.4M/1.50G [00:03<02:25, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   2%|‚ñç                  | 31.2M/1.50G [00:03<02:17, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   3%|‚ñå                  | 48.3M/1.50G [00:03<00:59, 24.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   4%|‚ñã                  | 54.3M/1.50G [00:03<01:10, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   7%|‚ñà‚ñç                  | 110M/1.50G [00:03<00:19, 72.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   9%|‚ñà‚ñã                  | 130M/1.50G [00:04<00:17, 78.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  10%|‚ñà‚ñâ                  | 149M/1.50G [00:04<00:14, 91.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  11%|‚ñà‚ñà‚ñè                 | 166M/1.50G [00:04<00:17, 77.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  13%|‚ñà‚ñà‚ñå                 | 191M/1.50G [00:04<00:17, 76.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  14%|‚ñà‚ñà‚ñã                 | 203M/1.50G [00:05<00:18, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  15%|‚ñà‚ñà‚ñâ                 | 221M/1.50G [00:05<00:19, 65.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  16%|‚ñà‚ñà‚ñà                 | 234M/1.50G [00:05<00:17, 72.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  16%|‚ñà‚ñà‚ñà‚ñé                | 246M/1.50G [00:05<00:19, 65.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  17%|‚ñà‚ñà‚ñà‚ñç                | 255M/1.50G [00:05<00:18, 68.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  19%|‚ñà‚ñà‚ñà‚ñã                | 279M/1.50G [00:06<00:12, 96.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  20%|‚ñà‚ñà‚ñà‚ñâ                | 294M/1.50G [00:06<00:19, 60.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  20%|‚ñà‚ñà‚ñà‚ñà                | 307M/1.50G [00:06<00:21, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  24%|‚ñà‚ñà‚ñà‚ñà‚ñâ                | 356M/1.50G [00:07<00:10, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 409M/1.50G [00:07<00:06, 164MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 434M/1.50G [00:07<00:06, 160MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 461M/1.50G [00:07<00:08, 122MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 486M/1.50G [00:07<00:07, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 514M/1.50G [00:07<00:06, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 556M/1.50G [00:08<00:05, 167MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 575M/1.50G [00:08<00:06, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 598M/1.50G [00:08<00:06, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 617M/1.50G [00:08<00:08, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 682M/1.50G [00:08<00:04, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 712M/1.50G [00:09<00:05, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 754M/1.50G [00:09<00:04, 164MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 827M/1.50G [00:09<00:03, 215MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 865M/1.50G [00:09<00:02, 224MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 898M/1.50G [00:10<00:02, 209MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 1.03G/1.50G [00:10<00:01, 345MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 1.12G/1.50G [00:10<00:00, 396MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1.25G/1.50G [00:10<00:00, 329MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.50G/1.50G [00:11<00:00, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:11<00:00,  1.71s/it]\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.61it/s]\n",
      "\n",
      "Saving tensors: 100% [======================================================================] 100 %\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W113 20:18:48.580935711 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n"
     ]
    }
   ],
   "source": [
    "# Define model and paths\n",
    "\n",
    "# Ensure storage path exists\n",
    "!mkdir -p {sllm_storage_path}\n",
    "\n",
    "# Run the conversion script\n",
    "# This script downloads the model (if needed) and converts it\n",
    "!python ServerlessLLM/sllm_store/examples/save_sglang_model.py \\\n",
    "   --model-name {model_name} \\\n",
    "   --storage-path {sllm_storage_path} \\\n",
    "   --dtype bfloat16 \\\n",
    "   --tensor-parallel-size 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6223a8-75f5-44e7-8fa9-6992aca69da0",
   "metadata": {},
   "source": [
    "## 3 Start the sllm-store server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59679745-ed1c-4384-8209-e1812552a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sllm-store started in background with PID: 1491768\n",
      "Check logs with: !tail -f sllm_server.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Ensure the storage path exists\n",
    "os.makedirs(sllm_storage_path, exist_ok=True)\n",
    "\n",
    "# Define the command as a list for safety\n",
    "cmd = [\n",
    "    \"sllm-store\", \"start\",\n",
    "    \"--storage-path\", sllm_storage_path,\n",
    "    \"--mem-pool-size\", \"4GB\"\n",
    "]\n",
    "\n",
    "# Open a log file to capture the output\n",
    "log_file = open(\"sllm_server.log\", \"w\")\n",
    "\n",
    "# Launch the process in the background\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=log_file,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    preexec_fn=os.setpgrp # Decouples the process from the notebook's process group\n",
    ")\n",
    "\n",
    "print(f\"sllm-store started in background with PID: {process.pid}\")\n",
    "print(\"Check logs with: !tail -f sllm_server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2bb7d",
   "metadata": {},
   "source": [
    "## 3. Performance Benchmark\n",
    "\n",
    "We compare the initialization time of the SGLang Engine using:\n",
    "1. **ServerlessLLM format** (`load_format=\"serverless_llm\"`)\n",
    "2. **Standard format** (`load_format=\"auto\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc5d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Benchmark...\n",
      "Loading Qwen/Qwen3-0.6B with ServerlessLLM format...\n",
      "/home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "DEBUG 01-13 20:49:39 torch.py:137] allocate_cuda_memory takes 0.0008206367492675781 seconds\n",
      "DEBUG 01-13 20:49:39 client.py:72] load_into_gpu: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 client.py:113] Model loaded: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 torch.py:160] restore state_dict takes 0.0004429817199707031 seconds\n",
      "INFO 01-13 20:49:39 client.py:117] confirm_model_loaded: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 client.py:125] Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing batches (bs=1 avail_mem=5.08 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:01<00:00,  6.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServerlessLLM load time: 26.0948s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on the green button logo they want tomas?\n",
      "A) Somewhere North\n",
      "B) Somewhere East\n",
      "C) Somewhere Tuesday\n",
      "D) Somewhere West\n",
      "E) None of the above?\n",
      "\n",
      "The choices are based on the logo tomas, which is a very well known French celebrity.\n",
      "Answer:\n",
      "C) Somewhere Tuesday\n",
      "\n",
      "The logo tomas is a famous and well-known French celebrity. On this logo, the shapes represent the like of a green button. The greenbutton logo is associated with the capital of France. On the green button logo they need tomas, the capital that would be represented is Somewhere Tuesday,\n",
      "Loading Qwen/Qwen3-0.6B with Standard format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]\n",
      "\n",
      "Capturing batches (bs=1 avail_mem=5.03 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:01<00:00,  6.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard load time: 39.2057s\n",
      "?\n",
      "\n",
      "The capital of France is Paris. LaTeX, the way you mentioned it, is used to write mathematical formulas. So, if I were to write the capital of France in LaTeX, it would be denoted by the symbol \\text{Paris}. However, I need to be careful. The capital of France is also known as Paris, which is sometimes also called the capital of the country. Therefore, you could use either \\text{Paris} or \\text{Paris}, both would be correct. The square brackets in Latex are often used to denote the capitals of different countries, but for Paris, it's better to use the\n",
      "RESULTS\n",
      "ServerlessLLM: 26.0948s\n",
      "Standard:      39.2057s\n",
      "Speedup:       1.50x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()\n",
    "def run_benchmark():\n",
    "    print(\"Starting Benchmark...\") \n",
    "    \n",
    "    # 1. Test ServerlessLLM format\n",
    "    print(f\"Loading {model_name} with ServerlessLLM format...\")\n",
    "    start = time.time()\n",
    "    # Note: model_path for sllm format is the local directory\n",
    "    sllm_model_path = os.path.join(sllm_storage_path, model_name)\n",
    "    print(sllm_model_path)\n",
    "    \n",
    "    try:\n",
    "        engine_sllm = Engine(\n",
    "            model_path=sllm_model_path,\n",
    "            load_format=\"serverless_llm\",\n",
    "            tp_size=1,\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        sllm_load_time = time.time() - start\n",
    "        print(f\"ServerlessLLM load time: {sllm_load_time:.4f}s\")\n",
    "        prompts = [\"What is the capital of France\"]\n",
    "        # engine.generate returns a list of dictionaries/objects depending on the SGLang version\n",
    "        outputs = engine_sllm.generate(prompts)\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Depending on SGLang version, output might be a dict or an object\n",
    "            if isinstance(output, dict):\n",
    "                print(output.get(\"text\", \"\"))\n",
    "            else:\n",
    "                print(output.text)\n",
    "    \n",
    "        # 3. Shutdown\n",
    "        engine_sllm.shutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"ServerlessLLM loading failed: {e}\")\n",
    "        sllm_load_time = float('inf')\n",
    "\n",
    "    \n",
    "        \n",
    "    # Clear GPU memory if possible\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 2. Test Standard format\n",
    "    print(f\"Loading {model_name} with Standard format...\")\n",
    "    hf_model_path = os.path.join(hf_storage_path, model_name)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        engine_std = Engine(\n",
    "            model_path=hf_model_path,\n",
    "            load_format=\"auto\",\n",
    "            tp_size=1,\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        std_load_time = time.time() - start\n",
    "        print(f\"Standard load time: {std_load_time:.4f}s\")\n",
    "        # Run inference    \n",
    "        prompts = [\"What is the capital of France\"]\n",
    "        # engine.generate returns a list of dictionaries/objects depending on the SGLang version\n",
    "        outputs = engine_std.generate(prompts)\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Depending on SGLang version, output might be a dict or an object\n",
    "            if isinstance(output, dict):\n",
    "                print(output.get(\"text\", \"\"))\n",
    "            else:\n",
    "                print(output.text)\n",
    "\n",
    "        # 3. Shutdown\n",
    "        engine_std.shutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"Standard loading failed: {e}\")\n",
    "        std_load_time = float('inf')\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    print(\"RESULTS\")\n",
    "    print(f\"ServerlessLLM: {sllm_load_time:.4f}s\")\n",
    "    print(f\"Standard:      {std_load_time:.4f}s\")\n",
    "    if sllm_load_time > 0 and std_load_time != float('inf'):\n",
    "        print(f\"Speedup:       {std_load_time/sllm_load_time:.2f}x\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Simple check to avoid running if imported\n",
    "    run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e54b51-f79b-4d11-a767-04b850058e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
