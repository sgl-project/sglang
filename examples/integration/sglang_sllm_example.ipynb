{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc205f35",
   "metadata": {},
   "source": [
    "# SGLang + ServerlessLLM Integration Example\n",
    "\n",
    "This notebook demonstrates the integration between SGLang and ServerlessLLM for optimized model loading and inference.\n",
    "\n",
    "We will:\n",
    "1. Install dependencies (uv, ServerlessLLM, SGLang)\n",
    "2. Convert a HuggingFace model (`Qwen/Qwen3-0.6B`) to ServerlessLLM format\n",
    "3. Run a benchmark comparing the loading time of the ServerlessLLM format vs. the standard format\n",
    "\n",
    "## Installation\n",
    "\n",
    "### 1. Install uv (recommended for faster installation)\n",
    "\n",
    "```bash\n",
    "pip install uv\n",
    "uv venv -p 3.10\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "### 2. Install SGLang\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/sgl-project/sglang.git\n",
    "\n",
    "# Install SGLang\n",
    "uv pip install -e sglang/python\n",
    "```\n",
    "\n",
    "### 3. Install ServerlessLLM\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/ServerlessLLM/ServerlessLLM.git\n",
    "\n",
    "# Install sllm_store\n",
    "uv pip install -e ServerlessLLM/sllm_store\n",
    "\n",
    "# Install ServerlessLLM\n",
    "uv pip install -e ServerlessLLM\n",
    "\n",
    "# Install SGLang again to match the dependency version\n",
    "uv pip install -e sglang/python\n",
    "```\n",
    "\n",
    "### 4. Install Jupyter notebook\n",
    "\n",
    "```bash\n",
    "uv pip install jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a98408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ sllm installed: /root/xinyuan/sglang/examples/integration/ServerlessLLM/sllm\n",
      "✅ sllm_store installed: /root/xinyuan/sglang/examples/integration/ServerlessLLM/sllm_store/sllm_store\n",
      "✅ sglang installed: /root/xinyuan/sglang/examples/integration/sglang\n",
      "\n",
      "✅ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Verify installation\n",
    "def check_installation():\n",
    "    errors = []\n",
    "    \n",
    "    if not shutil.which(\"sllm\"):\n",
    "        errors.append(\"sllm not found\")\n",
    "    if not shutil.which(\"sllm-store\"):\n",
    "        errors.append(\"sllm-store not found\")\n",
    "    \n",
    "    try:\n",
    "        import sllm\n",
    "        import sllm_store\n",
    "        print(f\"✅ sllm installed: {os.path.dirname(sllm.__file__)}\")\n",
    "        print(f\"✅ sllm_store installed: {os.path.dirname(sllm_store.__file__)}\")\n",
    "    except ImportError as e:\n",
    "        errors.append(f\"Import error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        import sglang\n",
    "        if hasattr(sglang, '__file__') and sglang.__file__:\n",
    "            print(f\"✅ sglang installed: {os.path.dirname(sglang.__file__)}\")\n",
    "        elif hasattr(sglang, '__path__'):\n",
    "            print(f\"✅ sglang installed: {sglang.__path__[0]}\")\n",
    "    except ImportError as e:\n",
    "        errors.append(f\"sglang not installed: {e}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n❌ Missing dependencies:\")\n",
    "        for err in errors:\n",
    "            print(f\"   - {err}\")\n",
    "        print(\"\\nPlease follow the installation instructions above.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All dependencies installed!\")\n",
    "\n",
    "check_installation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77e53e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import time\n",
    "import torch\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from sglang.srt.entrypoints.engine import Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bf979",
   "metadata": {},
   "source": [
    "## 2. Convert Model to ServerlessLLM Format\n",
    "\n",
    "We use the `Qwen/Qwen3-0.6B` model for this demonstration. \n",
    "The conversion script optimizes the model checkpoints for fast loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e664933-af06-4a69-8ae6-35fda2df938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "sllm_storage_path = \"models\" # or your preferred path to store the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a76f5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]\n",
      "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
      "\n",
      "generation_config.json: 100%|██████████████████| 239/239 [00:00<00:00, 1.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 726/726 [00:00<00:00, 9.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer_config.json: 9.73kB [00:00, 26.0MB/s]A\n",
      "Fetching 7 files:  14%|███▊                       | 1/7 [00:00<00:01,  5.35it/s]\n",
      "\n",
      "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "merges.txt: 1.67MB [00:00, 21.8MB/s]                | 0.00/1.50G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "vocab.json: 2.78MB [00:00, 33.4MB/s]\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json:   0%|                      | 37.4k/11.4M [00:00<02:00, 94.5kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                   | 5.93M/1.50G [00:00<02:24, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|▏                  | 18.4M/1.50G [00:00<00:46, 31.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|▍                  | 34.0M/1.50G [00:00<00:29, 49.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|████▉                | 352M/1.50G [00:00<00:01, 692MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|██████▊              | 484M/1.50G [00:01<00:01, 774MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|████████▋            | 623M/1.50G [00:01<00:00, 910MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|██████████▌          | 756M/1.50G [00:01<00:00, 895MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|█████████████▎     | 1.05G/1.50G [00:01<00:00, 1.29GB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors: 100%|████████████████████| 1.50G/1.50G [00:01<00:00, 871MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 7 files: 100%|███████████████████████████| 7/7 [00:01<00:00,  3.56it/s]\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "WARNING:sglang.srt.server_args:TensorRT-LLM MHA only supports page_size of 16, 32 or 64, changing page_size from None to 64.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]\n",
      "\n",
      "Saving tensors: 100% [======================================================================] 100 %\n",
      "/root/xinyuan/sglang/examples/integration/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W126 05:22:37.691296327 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n"
     ]
    }
   ],
   "source": [
    "# Ensure storage path exists\n",
    "!mkdir -p {sllm_storage_path}\n",
    "\n",
    "# Run the conversion script\n",
    "# This script downloads the model (if needed) and converts it\n",
    "!uv run python ServerlessLLM/sllm_store/examples/save_sglang_model.py \\\n",
    "   --model-name {model_name} \\\n",
    "   --storage-path {sllm_storage_path} \\\n",
    "   --dtype bfloat16 \\\n",
    "   --tensor-parallel-size 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6223a8-75f5-44e7-8fa9-6992aca69da0",
   "metadata": {},
   "source": [
    "## 3. Start the sllm-store server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59679745-ed1c-4384-8209-e1812552a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sllm-store started in background with PID: 781491\n",
      "Check logs with: !tail -f sllm_server.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command as a list for safety\n",
    "cmd = [\n",
    "    \"sllm-store\", \"start\",\n",
    "    \"--storage-path\", sllm_storage_path,\n",
    "    \"--mem-pool-size\", \"4GB\"\n",
    "]\n",
    "\n",
    "# Open a log file to capture the output\n",
    "log_file = open(\"sllm_server.log\", \"w\")\n",
    "\n",
    "# Launch the process in the background\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=log_file,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    preexec_fn=os.setpgrp # Decouples the process from the notebook's process group\n",
    ")\n",
    "\n",
    "print(f\"sllm-store started in background with PID: {process.pid}\")\n",
    "print(\"Check logs with: !tail -f sllm_server.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cd2ca-9b26-43e0-9f42-c71c9a6ddc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/xinyuan/sglang/examples/integration/.venv/bin/sllm-store\", line 4, in <module>\n",
      "    from sllm_store.cli import main\n",
      "  File \"/root/xinyuan/sglang/examples/integration/ServerlessLLM/sllm_store/sllm_store/cli.py\", line 30, in <module>\n",
      "    from sllm_store.server import serve\n",
      "  File \"/root/xinyuan/sglang/examples/integration/ServerlessLLM/sllm_store/sllm_store/server.py\", line 13, in <module>\n",
      "    ctypes.CDLL(os.path.join(sllm_store.__path__[0], \"libglog.so\"))\n",
      "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /root/xinyuan/sglang/examples/integration/ServerlessLLM/sllm_store/sllm_store/libglog.so: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!tail -f sllm_server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2bb7d",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmark\n",
    "\n",
    "We compare the initialization time of the SGLang Engine using:\n",
    "- **ServerlessLLM format** (`load_format=\"serverless_llm\"`)\n",
    "- **Standard format** (`load_format=\"auto\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc5d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Benchmark...\n",
      "Loading Qwen/Qwen3-0.6B with ServerlessLLM format...\n",
      "/home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "DEBUG 01-13 20:49:39 torch.py:137] allocate_cuda_memory takes 0.0008206367492675781 seconds\n",
      "DEBUG 01-13 20:49:39 client.py:72] load_into_gpu: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 client.py:113] Model loaded: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 torch.py:160] restore state_dict takes 0.0004429817199707031 seconds\n",
      "INFO 01-13 20:49:39 client.py:117] confirm_model_loaded: /home/users/ntu/ktang022/scratch/models/Qwen/Qwen3-0.6B/rank_0, 2e757c0a-15ec-4ea7-ac83-41f3334d0fb6\n",
      "INFO 01-13 20:49:39 client.py:125] Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing batches (bs=1 avail_mem=5.08 GB): 100%|██████████| 8/8 [00:01<00:00,  6.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServerlessLLM load time: 26.0948s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on the green button logo they want tomas?\n",
      "A) Somewhere North\n",
      "B) Somewhere East\n",
      "C) Somewhere Tuesday\n",
      "D) Somewhere West\n",
      "E) None of the above?\n",
      "\n",
      "The choices are based on the logo tomas, which is a very well known French celebrity.\n",
      "Answer:\n",
      "C) Somewhere Tuesday\n",
      "\n",
      "The logo tomas is a famous and well-known French celebrity. On this logo, the shapes represent the like of a green button. The greenbutton logo is associated with the capital of France. On the green button logo they need tomas, the capital that would be represented is Somewhere Tuesday,\n",
      "Loading Qwen/Qwen3-0.6B with Standard format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n",
      "/home/users/ntu/ktang022/scratch/envs/sllm/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]\n",
      "\n",
      "Capturing batches (bs=1 avail_mem=5.03 GB): 100%|██████████| 8/8 [00:01<00:00,  6.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard load time: 39.2057s\n",
      "?\n",
      "\n",
      "The capital of France is Paris. LaTeX, the way you mentioned it, is used to write mathematical formulas. So, if I were to write the capital of France in LaTeX, it would be denoted by the symbol \\text{Paris}. However, I need to be careful. The capital of France is also known as Paris, which is sometimes also called the capital of the country. Therefore, you could use either \\text{Paris} or \\text{Paris}, both would be correct. The square brackets in Latex are often used to denote the capitals of different countries, but for Paris, it's better to use the\n",
      "RESULTS\n",
      "ServerlessLLM: 26.0948s\n",
      "Standard:      39.2057s\n",
      "Speedup:       1.50x\n"
     ]
    }
   ],
   "source": [
    "def run_benchmark():\n",
    "    print(\"Starting Benchmark...\") \n",
    "    \n",
    "    # 1. Test ServerlessLLM format\n",
    "    print(f\"Loading {model_name} with ServerlessLLM format...\")\n",
    "    start = time.time()\n",
    "    # Note: model_path for sllm format is the local directory\n",
    "    sllm_model_path = os.path.join(sllm_storage_path, model_name)\n",
    "    print(sllm_model_path)\n",
    "    \n",
    "    try:\n",
    "        engine_sllm = Engine(\n",
    "            model_path=sllm_model_path,\n",
    "            load_format=\"serverless_llm\",\n",
    "            tp_size=1,\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        sllm_load_time = time.time() - start\n",
    "        print(f\"ServerlessLLM load time: {sllm_load_time:.4f}s\")\n",
    "        prompts = [\"What is the capital of France\"]\n",
    "        # engine.generate returns a list of dictionaries/objects depending on the SGLang version\n",
    "        outputs = engine_sllm.generate(prompts)\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Depending on SGLang version, output might be a dict or an object\n",
    "            if isinstance(output, dict):\n",
    "                print(output.get(\"text\", \"\"))\n",
    "            else:\n",
    "                print(output.text)\n",
    "    \n",
    "        # 3. Shutdown\n",
    "        engine_sllm.shutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"ServerlessLLM loading failed: {e}\")\n",
    "        sllm_load_time = float('inf')\n",
    "\n",
    "    \n",
    "        \n",
    "    # Clear GPU memory if possible\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 2. Test Standard format\n",
    "    print(f\"Loading {model_name} with Standard format...\")\n",
    "    start = time.time()\n",
    "    try:\n",
    "        engine_std = Engine(\n",
    "            model_path=model_name,\n",
    "            load_format=\"auto\",\n",
    "            tp_size=1,\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        std_load_time = time.time() - start\n",
    "        print(f\"Standard load time: {std_load_time:.4f}s\")\n",
    "        # Run inference    \n",
    "        prompts = [\"What is the capital of France\"]\n",
    "        # engine.generate returns a list of dictionaries/objects depending on the SGLang version\n",
    "        outputs = engine_std.generate(prompts)\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Depending on SGLang version, output might be a dict or an object\n",
    "            if isinstance(output, dict):\n",
    "                print(output.get(\"text\", \"\"))\n",
    "            else:\n",
    "                print(output.text)\n",
    "\n",
    "        # 3. Shutdown\n",
    "        engine_std.shutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"Standard loading failed: {e}\")\n",
    "        std_load_time = float('inf')\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    print(\"RESULTS\")\n",
    "    print(f\"ServerlessLLM: {sllm_load_time:.4f}s\")\n",
    "    print(f\"Standard:      {std_load_time:.4f}s\")\n",
    "    if sllm_load_time > 0 and std_load_time != float('inf'):\n",
    "        print(f\"Speedup:       {std_load_time/sllm_load_time:.2f}x\")\n",
    "\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e54b51-f79b-4d11-a767-04b850058e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
