"""
Advanced analysis tools for kernel shape logs.

This script provides detailed analysis of shape logs generated by torch_shape_logger.py

Usage:
    python analyze_shapes.py <log_file.jsonl> [options]
    
Examples:
    # Basic analysis
    python analyze_shapes.py qwen_kernel_shapes.jsonl
    
    # Show top operations with shape details
    python analyze_shapes.py qwen_kernel_shapes.jsonl --top 20 --show-shapes
    
    # Find operations with large tensors
    python analyze_shapes.py qwen_kernel_shapes.jsonl --min-elements 1000000
    
    # Export operation timeline
    python analyze_shapes.py qwen_kernel_shapes.jsonl --timeline timeline.csv
"""

import argparse
import json
from collections import defaultdict
from typing import Any, Dict, List, Tuple


def load_log_file(log_file: str) -> List[Dict[str, Any]]:
    """Load JSONL log file."""
    entries = []
    with open(log_file, "r") as f:
        for line in f:
            try:
                entries.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Warning: Failed to parse line: {e}")
    return entries


def count_tensor_elements(shape_info) -> int:
    """Count number of elements in a tensor from shape info."""
    if isinstance(shape_info, dict) and "shape" in shape_info:
        shape = shape_info["shape"]
        if shape:
            result = 1
            for dim in shape:
                result *= dim
            return result
    elif isinstance(shape_info, list):
        if all(isinstance(x, int) for x in shape_info):
            # This is a shape list
            if shape_info:
                result = 1
                for dim in shape_info:
                    result *= dim
                return result
        else:
            # List of shapes
            return sum(count_tensor_elements(item) for item in shape_info)
    return 0


def analyze_operations(entries: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze operation patterns."""
    op_counts = defaultdict(int)
    op_shapes = defaultdict(list)
    op_total_elements = defaultdict(int)

    for entry in entries:
        op_name = entry["operation"]
        op_counts[op_name] += 1

        # Track output shapes
        if entry.get("outputs"):
            op_shapes[op_name].append(entry["outputs"])
            # Count elements
            elements = count_tensor_elements(entry["outputs"])
            op_total_elements[op_name] += elements

    return {
        "op_counts": dict(op_counts),
        "op_shapes": dict(op_shapes),
        "op_total_elements": dict(op_total_elements),
    }


def find_largest_operations(entries: List[Dict[str, Any]], top_n: int = 10) -> List[Tuple[int, str, int]]:
    """Find operations with the largest tensor outputs."""
    operation_sizes = []

    for entry in entries:
        elements = count_tensor_elements(entry.get("outputs"))
        if elements > 0:
            operation_sizes.append((entry["call_id"], entry["operation"], elements))

    return sorted(operation_sizes, key=lambda x: x[2], reverse=True)[:top_n]


def find_unique_shapes(entries: List[Dict[str, Any]]) -> Dict[str, set]:
    """Find unique shapes for each operation type."""
    unique_shapes = defaultdict(set)

    for entry in entries:
        op_name = entry["operation"]
        output_str = json.dumps(entry.get("outputs"), sort_keys=True)
        unique_shapes[op_name].add(output_str)

    return {op: len(shapes) for op, shapes in unique_shapes.items()}


def generate_timeline(entries: List[Dict[str, Any]], output_file: str):
    """Generate a CSV timeline of operations."""
    import csv

    with open(output_file, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["call_id", "operation", "op_count", "output_elements", "output_shape"])

        for entry in entries:
            elements = count_tensor_elements(entry.get("outputs"))
            shape_str = json.dumps(entry.get("outputs"))
            writer.writerow([
                entry["call_id"],
                entry["operation"],
                entry.get("op_count", 0),
                elements,
                shape_str,
            ])

    print(f"Timeline exported to: {output_file}")


def print_operation_summary(analysis: Dict[str, Any], top_n: int = 20):
    """Print a summary of operations."""
    print("\n" + "=" * 80)
    print(f"Top {top_n} Most Frequent Operations")
    print("=" * 80)

    sorted_ops = sorted(analysis["op_counts"].items(), key=lambda x: x[1], reverse=True)[:top_n]

    print(f"{'Count':>10} | {'Total Elements':>15} | {'Operation'}")
    print("-" * 80)

    for op_name, count in sorted_ops:
        total_elements = analysis["op_total_elements"].get(op_name, 0)
        print(f"{count:>10} | {total_elements:>15,} | {op_name}")


def print_largest_operations(entries: List[Dict[str, Any]], top_n: int = 10):
    """Print operations with largest tensor outputs."""
    print("\n" + "=" * 80)
    print(f"Top {top_n} Operations by Tensor Size")
    print("=" * 80)

    largest = find_largest_operations(entries, top_n)

    print(f"{'Call ID':>8} | {'Elements':>15} | {'Operation'}")
    print("-" * 80)

    for call_id, op_name, elements in largest:
        print(f"{call_id:>8} | {elements:>15,} | {op_name}")


def print_shape_diversity(entries: List[Dict[str, Any]]):
    """Print shape diversity statistics."""
    print("\n" + "=" * 80)
    print("Shape Diversity (Operations with Most Unique Shapes)")
    print("=" * 80)

    unique_shapes = find_unique_shapes(entries)
    sorted_diversity = sorted(unique_shapes.items(), key=lambda x: x[1], reverse=True)[:20]

    print(f"{'Unique Shapes':>15} | {'Operation'}")
    print("-" * 80)

    for op_name, count in sorted_diversity:
        print(f"{count:>15} | {op_name}")


def filter_by_operation(entries: List[Dict[str, Any]], pattern: str) -> List[Dict[str, Any]]:
    """Filter entries by operation name pattern."""
    return [e for e in entries if pattern.lower() in e["operation"].lower()]


def filter_by_min_elements(entries: List[Dict[str, Any]], min_elements: int) -> List[Dict[str, Any]]:
    """Filter entries with output tensors having at least min_elements."""
    filtered = []
    for entry in entries:
        elements = count_tensor_elements(entry.get("outputs"))
        if elements >= min_elements:
            filtered.append(entry)
    return filtered


def print_detailed_shapes(entries: List[Dict[str, Any]], max_entries: int = 10):
    """Print detailed shape information for selected entries."""
    print("\n" + "=" * 80)
    print(f"Detailed Shape Information (showing {max_entries} entries)")
    print("=" * 80)

    for i, entry in enumerate(entries[:max_entries]):
        print(f"\n[{entry['call_id']}] {entry['operation']} (occurrence #{entry.get('op_count', '?')})")

        if entry.get("inputs"):
            inputs_str = json.dumps(entry['inputs'], indent=2)
            print(f"  Inputs:  {inputs_str}")

        if entry.get("outputs"):
            outputs_str = json.dumps(entry['outputs'], indent=2)
            print(f"  Outputs: {outputs_str}")

        elements = count_tensor_elements(entry.get("outputs"))
        if elements > 0:
            print(f"  Elements: {elements:,}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze kernel shape logs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("log_file", help="Path to JSONL log file")
    parser.add_argument("--top", type=int, default=20, help="Number of top operations to show (default: 20)")
    parser.add_argument("--show-shapes", action="store_true", help="Show detailed shape information")
    parser.add_argument("--max-shapes", type=int, default=10, help="Max shapes to show in detail (default: 10)")
    parser.add_argument("--filter-op", type=str, help="Filter by operation name pattern")
    parser.add_argument("--min-elements", type=int, help="Filter operations with min tensor elements")
    parser.add_argument("--timeline", type=str, help="Export timeline to CSV file")
    parser.add_argument("--no-summary", action="store_true", help="Skip operation summary")
    parser.add_argument("--no-largest", action="store_true", help="Skip largest operations")
    parser.add_argument("--no-diversity", action="store_true", help="Skip shape diversity analysis")

    args = parser.parse_args()

    print(f"Loading log file: {args.log_file}")
    entries = load_log_file(args.log_file)
    print(f"Loaded {len(entries)} entries")

    # Apply filters
    if args.filter_op:
        entries = filter_by_operation(entries, args.filter_op)
        print(f"Filtered to {len(entries)} entries matching '{args.filter_op}'")

    if args.min_elements:
        entries = filter_by_min_elements(entries, args.min_elements)
        print(f"Filtered to {len(entries)} entries with >= {args.min_elements:,} elements")

    if not entries:
        print("No entries to analyze after filtering!")
        return

    # Analyze
    analysis = analyze_operations(entries)

    # Print results
    if not args.no_summary:
        print_operation_summary(analysis, args.top)

    if not args.no_largest:
        print_largest_operations(entries, args.top)

    if not args.no_diversity:
        print_shape_diversity(entries)

    if args.show_shapes:
        print_detailed_shapes(entries, args.max_shapes)

    # Export timeline
    if args.timeline:
        generate_timeline(entries, args.timeline)

    # Final statistics
    print("\n" + "=" * 80)
    print("Summary Statistics")
    print("=" * 80)
    print(f"Total operations:  {len(entries)}")
    print(f"Unique operations: {len(analysis['op_counts'])}")

    total_elements = sum(analysis["op_total_elements"].values())
    print(f"Total tensor elements processed: {total_elements:,}")

    # Find most common operation
    if analysis["op_counts"]:
        most_common = max(analysis["op_counts"].items(), key=lambda x: x[1])
        print(f"Most common operation: {most_common[0]} ({most_common[1]} times)")


if __name__ == "__main__":
    main()
