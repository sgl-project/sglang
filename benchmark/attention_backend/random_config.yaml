global:
  client_cmd: python3 -m sglang.bench_serving --backend sglang --dataset-name random --request-rate 16 --random-input-len {random_input_len} --random-output-len {random_output_len}
  server_cmd: python3 -m sglang.launch_server --model-path {model_path} --disable-radix-cache --attention-backend {attn_backend} {cudagraph}

attn_backends:
  - fa3
  - flashinfer

models:
  - meta-llama/Llama-3.1-8B-Instruct

cudagraphs:
  - False
  - True

in_out_len_pairs:
  - [1000, 1000]
  - [5000, 1000]
  - [5000, 5000]
