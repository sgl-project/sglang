{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Sending Requests\n",
    "\n",
    "This notebook provides a quick-start guide for using SGLang after installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Launch a server\n",
    "\n",
    "This code block is equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0\n",
    "```\n",
    "\n",
    "in your command line and wait for the server to be ready. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T02:46:13.611212Z",
     "iopub.status.busy": "2024-11-01T02:46:13.611093Z",
     "iopub.status.idle": "2024-11-01T02:46:42.810261Z",
     "shell.execute_reply": "2024-11-01T02:46:42.809147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-01 21:11:18] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct', chat_template=None, is_embedding=False, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=991460189, constrained_json_whitespace_pattern=None, decode_log_interval=40, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, watchdog_timeout=600, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_penalizer=False, disable_nan_detection=False, enable_overlap_schedule=False, enable_mixed_chunk=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1)\n",
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-01 21:11:24 TP0] Init torch distributed begin.\n",
      "[2024-11-01 21:11:25 TP0] Load weight begin. avail mem=47.27 GB\n",
      "[2024-11-01 21:11:26 TP0] lm_eval is not installed, GPTQ may not be usable\n",
      "INFO 11-01 21:11:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.30it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.19it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  3.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.74it/s]\n",
      "\n",
      "[2024-11-01 21:11:29 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=32.22 GB\n",
      "[2024-11-01 21:11:29 TP0] Memory pool end. avail mem=4.60 GB\n",
      "[2024-11-01 21:11:29 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "[2024-11-01 21:11:37 TP0] max_total_num_tokens=217512, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n",
      "[2024-11-01 21:11:38] INFO:     Started server process [3275517]\n",
      "[2024-11-01 21:11:38] INFO:     Waiting for application startup.\n",
      "[2024-11-01 21:11:38] INFO:     Application startup complete.\n",
      "[2024-11-01 21:11:38] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)\n",
      "[2024-11-01 21:11:38] INFO:     127.0.0.1:55626 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:11:39] INFO:     127.0.0.1:55634 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:11:39 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:11:39] INFO:     127.0.0.1:55650 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:11:39] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "\"\"\"\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(\"http://localhost:30000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl or requests. The server implements the [OpenAI-compatible API](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "### Using curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:11:43 TP0] Prefill batch. #new-seq: 1, #new-token: 46, #cached-token: 1, cache hit rate: 1.85%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:11:44 TP0] Decode batch. #running-req: 1, #token: 80, token usage: 0.00, gen throughput (token/s): 5.80, #queue-req: 0\n",
      "[2024-11-01 21:11:45 TP0] Decode batch. #running-req: 1, #token: 120, token usage: 0.00, gen throughput (token/s): 42.46, #queue-req: 0\n",
      "[2024-11-01 21:11:46 TP0] Decode batch. #running-req: 1, #token: 160, token usage: 0.00, gen throughput (token/s): 42.40, #queue-req: 0\n",
      "[2024-11-01 21:11:47 TP0] Decode batch. #running-req: 1, #token: 200, token usage: 0.00, gen throughput (token/s): 42.38, #queue-req: 0\n",
      "[2024-11-01 21:11:48 TP0] Decode batch. #running-req: 1, #token: 240, token usage: 0.00, gen throughput (token/s): 42.38, #queue-req: 0\n",
      "[2024-11-01 21:11:49 TP0] Decode batch. #running-req: 1, #token: 280, token usage: 0.00, gen throughput (token/s): 42.34, #queue-req: 0\n",
      "[2024-11-01 21:11:50 TP0] Decode batch. #running-req: 1, #token: 320, token usage: 0.00, gen throughput (token/s): 42.32, #queue-req: 0\n",
      "[2024-11-01 21:11:51 TP0] Decode batch. #running-req: 1, #token: 360, token usage: 0.00, gen throughput (token/s): 42.32, #queue-req: 0\n",
      "[2024-11-01 21:11:51 TP0] Decode batch. #running-req: 1, #token: 400, token usage: 0.00, gen throughput (token/s): 42.33, #queue-req: 0\n",
      "[2024-11-01 21:11:52 TP0] Decode batch. #running-req: 1, #token: 440, token usage: 0.00, gen throughput (token/s): 42.24, #queue-req: 0\n",
      "[2024-11-01 21:11:53 TP0] Decode batch. #running-req: 1, #token: 480, token usage: 0.00, gen throughput (token/s): 42.14, #queue-req: 0\n",
      "[2024-11-01 21:11:54 TP0] Decode batch. #running-req: 1, #token: 520, token usage: 0.00, gen throughput (token/s): 42.16, #queue-req: 0\n",
      "[2024-11-01 21:11:54] INFO:     127.0.0.1:55652 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{'id': 'c2b73de7a95d4ac9a851a8eafba80e20', 'object': 'chat.completion', 'created': 1730520714, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and generate human-like language. LLMs are trained on vast amounts of text data, which allows them to learn patterns, relationships, and context within language.\\n\\nLarge Language Models are typically trained using a technique called deep learning, specifically a type of neural network called a transformer. This allows them to understand and generate text at a level that's often indistinguishable from a human writer.\\n\\nSome key characteristics of LLMs include:\\n\\n1. **Language understanding**: LLMs can comprehend and interpret language, including nuances like idioms, sarcasm, and figurative language.\\n2. **Text generation**: LLMs can generate text based on a prompt or input, often creating coherent and contextually relevant content.\\n3. **Contextual understanding**: LLMs can understand the relationships between words, phrases, and sentences within a larger piece of text.\\n4. **Self-supervised learning**: LLMs can learn from large amounts of text data without explicit supervision or labeling.\\n\\nLLMs have many applications, including:\\n\\n1. **Chatbots and virtual assistants**: LLMs power conversational interfaces like Siri, Alexa, and Google Assistant.\\n2. **Language translation**: LLMs can translate text and speech from one language to another.\\n3. **Content generation**: LLMs can generate articles, blog posts, and even entire books.\\n4. **Question-answering**: LLMs can answer questions based on their training data.\\n\\nExamples of popular LLMs include:\\n\\n1. **BERT** (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT is a widely used LLM for natural language processing tasks.\\n2. **RoBERTa** (Robustly Optimized BERT Pretraining Approach): An improved version of BERT, RoBERTa is also developed by Google.\\n3. **ChatGPT**: A conversational AI developed by OpenAI, ChatGPT is a popular LLM-based chatbot.\\n4. **LLaMA** (Large Language Model Meta AI): Developed by Meta AI, LLaMA is a large language model that's part of the Llama2 model family.\\n\\nOverall, LLMs are powerful AI systems that have the potential to revolutionize many areas of human communication and interaction.\"}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 128009}], 'usage': {'prompt_tokens': 47, 'total_tokens': 525, 'completion_tokens': 478, 'prompt_tokens_details': None}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess, json\n",
    "\n",
    "curl_command = \"\"\"\n",
    "curl -s http://localhost:30000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer None\" \\\n",
    "  -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'\n",
    "\"\"\"\n",
    "\n",
    "response = json.loads(subprocess.check_output(curl_command, shell=True))\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T02:46:42.813656Z",
     "iopub.status.busy": "2024-11-01T02:46:42.813354Z",
     "iopub.status.idle": "2024-11-01T02:46:51.436613Z",
     "shell.execute_reply": "2024-11-01T02:46:51.435965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:11:54 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 46, cache hit rate: 46.53%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:11:55 TP0] Decode batch. #running-req: 1, #token: 83, token usage: 0.00, gen throughput (token/s): 40.46, #queue-req: 0\n",
      "[2024-11-01 21:11:56 TP0] Decode batch. #running-req: 1, #token: 123, token usage: 0.00, gen throughput (token/s): 42.49, #queue-req: 0\n",
      "[2024-11-01 21:11:57 TP0] Decode batch. #running-req: 1, #token: 163, token usage: 0.00, gen throughput (token/s): 42.39, #queue-req: 0\n",
      "[2024-11-01 21:11:58 TP0] Decode batch. #running-req: 1, #token: 203, token usage: 0.00, gen throughput (token/s): 42.38, #queue-req: 0\n",
      "[2024-11-01 21:11:59 TP0] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, gen throughput (token/s): 42.34, #queue-req: 0\n",
      "[2024-11-01 21:12:00 TP0] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, gen throughput (token/s): 42.32, #queue-req: 0\n",
      "[2024-11-01 21:12:01 TP0] Decode batch. #running-req: 1, #token: 323, token usage: 0.00, gen throughput (token/s): 42.29, #queue-req: 0\n",
      "[2024-11-01 21:12:02 TP0] Decode batch. #running-req: 1, #token: 363, token usage: 0.00, gen throughput (token/s): 42.30, #queue-req: 0\n",
      "[2024-11-01 21:12:03] INFO:     127.0.0.1:35416 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{'id': '194a9008bf444d3fb2417197e847318b', 'object': 'chat.completion', 'created': 1730520723, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and generate human-like language. LLMs are trained on vast amounts of text data, which allows them to learn patterns, relationships, and nuances of language.\\n\\nLarge Language Models like myself are capable of:\\n\\n1. **Understanding natural language**: We can comprehend and interpret human language, including idioms, colloquialisms, and context-dependent expressions.\\n2. **Generating text**: We can create coherent and relevant text based on a given prompt, topic, or style.\\n3. **Answering questions**: We can provide accurate and informative responses to a wide range of questions, from simple facts to complex topics.\\n4. **Translation**: We can translate text from one language to another, including popular languages such as Spanish, French, German, Chinese, and many more.\\n5. **Summarization**: We can condense long pieces of text into concise summaries, highlighting key points and main ideas.\\n\\nLLMs like myself are often used in various applications, including:\\n\\n1. **Virtual assistants**: We can help with tasks, answer questions, and provide information.\\n2. **Language translation**: We can assist with translating text and speech in real-time.\\n3. **Content generation**: We can help create articles, blog posts, social media content, and more.\\n4. **Research and analysis**: We can assist with data analysis, research, and academic writing.\\n5. **Customer service**: We can provide 24/7 support and answer customer queries.\\n\\nKeep in mind that while LLMs are incredibly powerful tools, they are not perfect and may make mistakes. However, they continue to improve with each new update and training iteration.\"}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 128009}], 'usage': {'prompt_tokens': 47, 'total_tokens': 396, 'completion_tokens': 349, 'prompt_tokens_details': None}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:30000/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is a LLM?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client\n",
    "\n",
    "You can also use the OpenAI Python API library to send requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T02:46:51.439372Z",
     "iopub.status.busy": "2024-11-01T02:46:51.439178Z",
     "iopub.status.idle": "2024-11-01T02:46:52.895776Z",
     "shell.execute_reply": "2024-11-01T02:46:52.895318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:12:03 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 29, cache hit rate: 50.67%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:12:03 TP0] Decode batch. #running-req: 1, #token: 57, token usage: 0.00, gen throughput (token/s): 29.21, #queue-req: 0\n",
      "[2024-11-01 21:12:04] INFO:     127.0.0.1:35420 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>ChatCompletion(id='6dced81e28a747b78437e431f88466f7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **Country:** Japan\\n**Capital:** Tokyo\\n\\n2. **Country:** Australia\\n**Capital:** Canberra\\n\\n3. **Country:** Brazil\\n**Capital:** Bras√≠lia', refusal=None, role='assistant', function_call=None, tool_calls=None), matched_stop=128009)], created=1730520724, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=49, total_tokens=95, prompt_tokens_details=None))</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T02:46:52.898411Z",
     "iopub.status.busy": "2024-11-01T02:46:52.898149Z",
     "iopub.status.idle": "2024-11-01T02:46:54.398382Z",
     "shell.execute_reply": "2024-11-01T02:46:54.397564Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
