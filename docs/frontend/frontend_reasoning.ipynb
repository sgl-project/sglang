{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server with a reasoning model and reasoning parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/sglang/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:37:42] server_args=ServerArgs(model_path='Qwen/Qwen3-4B', tokenizer_path='Qwen/Qwen3-4B', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen3-4B', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=38453, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=106689967, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='qwen3', dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\n",
      "[2025-05-04 18:37:48] Attention backend not set. Use flashinfer backend by default.\n",
      "[2025-05-04 18:37:48] Init torch distributed begin.\n",
      "[2025-05-04 18:37:48] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-05-04 18:37:48] Load weight begin. avail mem=43.89 GB\n",
      "[2025-05-04 18:37:49] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  4.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.53it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.73it/s]\n",
      "\n",
      "[2025-05-04 18:37:50] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=36.25 GB, mem usage=7.63 GB.\n",
      "[2025-05-04 18:37:50] KV Cache is allocated. #tokens: 225647, K size: 15.49 GB, V size: 15.49 GB\n",
      "[2025-05-04 18:37:50] Memory pool end. avail mem=4.71 GB\n",
      "2025-05-04 18:37:50,845 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "[2025-05-04 18:37:50] Capture cuda graph begin. This can take up to several minutes. avail mem=4.09 GB\n",
      "[2025-05-04 18:37:50] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]\n",
      "Capturing batches (avail_mem=4.06 GB):   0%|          | 0/23 [00:00<?, ?it/s]2025-05-04 18:37:51,316 - INFO - flashinfer.jit: Loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n",
      "2025-05-04 18:37:51,338 - INFO - flashinfer.jit: Finished loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n",
      "Capturing batches (avail_mem=2.68 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:06<00:00,  3.69it/s]\n",
      "[2025-05-04 18:37:57] Capture cuda graph end. Time elapsed: 6.28 s. mem usage=1.41 GB. avail mem=2.67 GB.\n",
      "[2025-05-04 18:37:57] max_total_num_tokens=225647, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2821, context_len=40960\n",
      "[2025-05-04 18:37:57] INFO:     Started server process [764462]\n",
      "[2025-05-04 18:37:57] INFO:     Waiting for application startup.\n",
      "[2025-05-04 18:37:57] INFO:     Application startup complete.\n",
      "[2025-05-04 18:37:57] INFO:     Uvicorn running on http://0.0.0.0:38453 (Press CTRL+C to quit)\n",
      "[2025-05-04 18:37:58] INFO:     127.0.0.1:48318 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-05-04 18:37:58] INFO:     127.0.0.1:48330 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-05-04 18:37:58] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "2025-05-04 18:37:59,601 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-05-04 18:37:59,623 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "[2025-05-04 18:37:59] INFO:     127.0.0.1:48342 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-05-04 18:37:59] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.\n",
      "                    \n",
      "Server started on http://localhost:38453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sglang import separate_reasoning, assistant_begin, assistant_end\n",
    "from sglang import assistant, function, gen, system, user\n",
    "from sglang import image\n",
    "from sglang import RuntimeEndpoint, set_default_backend\n",
    "from sglang.srt.utils import load_image\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "from sglang.utils import print_highlight, terminate_process, wait_for_server\n",
    "\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen3-4B --reasoning-parser qwen3 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")\n",
    "print(f\"Server started on http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default backend. Note: you can set chat_template_name in RontimeEndpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:38:07] INFO:     127.0.0.1:35064 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "set_default_backend(RuntimeEndpoint(f\"http://localhost:{port}\", chat_template_name=\"qwen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:38:08] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-04 18:38:08] Decode batch. #running-req: 1, #token: 64, token usage: 0.00, gen throughput (token/s): 3.63, #queue-req: 0\n",
      "[2025-05-04 18:38:09] Decode batch. #running-req: 1, #token: 104, token usage: 0.00, gen throughput (token/s): 82.06, #queue-req: 0\n",
      "[2025-05-04 18:38:09] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, gen throughput (token/s): 81.59, #queue-req: 0\n",
      "[2025-05-04 18:38:10] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, gen throughput (token/s): 81.13, #queue-req: 0\n",
      "[2025-05-04 18:38:10] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, gen throughput (token/s): 80.87, #queue-req: 0\n",
      "[2025-05-04 18:38:11] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, gen throughput (token/s): 80.54, #queue-req: 0\n",
      "[2025-05-04 18:38:11] INFO:     127.0.0.1:35066 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def basic_qa(s, question):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(question)\n",
    "    s += assistant_begin()\n",
    "    s += gen(\"answer\", max_tokens=512)\n",
    "    s += assistant_end()\n",
    "\n",
    "state = basic_qa(\"List 3 countries and their capitals.\")\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for three countries and their capitals. Let me think about which countries to pick. I should choose well-known ones so that the information is accurate and commonly recognized.\n",
      "\n",
      "First, France. Paris is its capital, and that's a major city, so that's a good start. Next, maybe Japan. Tokyo is the capital, and it's a big country with a large population. Then, Brazil. Bras√≠lia is the capital, but I should make sure I get the correct spelling. Wait, yes, Bras√≠lia is the capital. Alternatively, maybe Canada? Ottawa is the capital. But I think France, Japan, and Brazil are all good examples. Alternatively, the United States with Washington D.C. But maybe the user wants a variety of regions. Let me confirm the capitals again. France: Paris. Japan: Tokyo. Brazil: Bras√≠lia. Yes, that's correct. I should present them clearly.\n",
      "</think>\n",
      "\n",
      "Here are three countries and their capitals:  \n",
      "1. **France** ‚Äì **Paris**  \n",
      "2. **Japan** ‚Äì **Tokyo**  \n",
      "3. **Brazil** ‚Äì **Bras√≠lia**  \n",
      "\n",
      "Let me know if you'd like more examples! üåç\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print as raw text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['answer', 'answer_reasoning_content'])\n",
      "[2025-05-04 18:40:56] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-04 18:40:56] Decode batch. #running-req: 1, #token: 42, token usage: 0.00, gen throughput (token/s): 1.01, #queue-req: 0\n",
      "[2025-05-04 18:40:57] Decode batch. #running-req: 1, #token: 82, token usage: 0.00, gen throughput (token/s): 82.29, #queue-req: 0\n",
      "[2025-05-04 18:40:57] Decode batch. #running-req: 1, #token: 122, token usage: 0.00, gen throughput (token/s): 81.90, #queue-req: 0\n",
      "[2025-05-04 18:40:58] Decode batch. #running-req: 1, #token: 162, token usage: 0.00, gen throughput (token/s): 81.36, #queue-req: 0\n",
      "[2025-05-04 18:40:58] Decode batch. #running-req: 1, #token: 202, token usage: 0.00, gen throughput (token/s): 81.05, #queue-req: 0\n",
      "[2025-05-04 18:40:59] INFO:     127.0.0.1:54664 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "Separated Reasoning Content:\n",
      "Okay, the user asked for three countries and their capitals. Let me think. I need to make sure I get the correct capitals for each country. First, I can start with a well-known country like France. Its capital is Paris. Then, maybe the United States, which has Washington, D.C. as its capital. That's a common one. Third, I should pick another country. How about Canada? Their capital is Ottawa. Wait, let me verify those to be sure. France is definitely Paris. The U.S. is Washington, D.C. Canada's capital is Ottawa. Yeah, those are all correct. I think that's three accurate examples. I should present them in a clear list format. Let me check if there's any other country that's more commonly known, but these three are straightforward. Yep, that should work.\n",
      "\n",
      "\n",
      "\n",
      "Content:\n",
      "1. **France** - Paris  \n",
      "2. **United States** - Washington, D.C.  \n",
      "3. **Canada** - Ottawa\n",
      "\n",
      "\n",
      "Messages:\n",
      "{'role': 'assistant', 'content': '1. **France** - Paris  \\n2. **United States** - Washington, D.C.  \\n3. **Canada** - Ottawa'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@function\n",
    "def basic_qa_separate_reasoning(s, question):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(question)\n",
    "    s += assistant_begin()\n",
    "    s += separate_reasoning(\n",
    "        gen(\"answer\", max_tokens=512),\n",
    "        model_type=\"qwen3\"\n",
    "    )\n",
    "    s += assistant_end()\n",
    "\n",
    "reasoning_state = basic_qa_separate_reasoning(\"List 3 countries and their capitals.\")\n",
    "print_highlight(reasoning_state.stream_executor.variable_event.keys())\n",
    "print_highlight(f\"\\nSeparated Reasoning Content:\\n{reasoning_state[\"answer_reasoning_content\"]}\")\n",
    "print_highlight(f\"\\n\\nContent:\\n{reasoning_state[\"answer\"]}\")\n",
    "print_highlight(f\"\\n\\nMessages:\\n{reasoning_state.messages()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Here are three countries and their capitals:  \\\\n1. **United States** ‚Äì Washington, D.C.  \\\\n2. **France** ‚Äì Paris  \\\\n3. **Germany** ‚Äì Berlin  \\\\n\\\\nLet me know if you\\'d like more examples!\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(reasoning_state['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:40:08] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 35, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-04 18:40:08] Decode batch. #running-req: 1, #token: 60, token usage: 0.00, gen throughput (token/s): 3.67, #queue-req: 0\n",
      "[2025-05-04 18:40:09] Decode batch. #running-req: 1, #token: 100, token usage: 0.00, gen throughput (token/s): 82.13, #queue-req: 0\n",
      "[2025-05-04 18:40:09] Decode batch. #running-req: 1, #token: 140, token usage: 0.00, gen throughput (token/s): 81.68, #queue-req: 0\n",
      "[2025-05-04 18:40:10] Decode batch. #running-req: 1, #token: 180, token usage: 0.00, gen throughput (token/s): 81.19, #queue-req: 0\n",
      "[2025-05-04 18:40:10] Decode batch. #running-req: 1, #token: 220, token usage: 0.00, gen throughput (token/s): 80.95, #queue-req: 0\n",
      "[2025-05-04 18:40:11] Decode batch. #running-req: 1, #token: 260, token usage: 0.00, gen throughput (token/s): 80.64, #queue-req: 0\n",
      "[2025-05-04 18:40:11] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, gen throughput (token/s): 80.25, #queue-req: 0\n",
      "[2025-05-04 18:40:11] INFO:     127.0.0.1:54490 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "first_answer:\n",
      "Here is a list of three countries and their capitals:\n",
      "\n",
      "1. **France** - **Paris**  \n",
      "2. **Japan** - **Tokyo**  \n",
      "3. **Brazil** - **Bras√≠lia**  \n",
      "\n",
      "Let me know if you'd like more examples!\n",
      "\n",
      "\n",
      "first_answer_reasoning_content:\n",
      "Okay, the user is asking for a list of three countries and their capitals. Let me think about which countries to choose. I should make sure they are well-known to avoid confusion. Maybe start with the most common ones.\n",
      "\n",
      "First, France. Its capital is Paris. That's a safe and well-known choice. Next, Japan. The capital is Tokyo. I remember that Tokyo is also a major economic hub, so that's good. Then, maybe Brazil. The capital is Bras√≠lia. Wait, is that right? I think I've heard that Bras√≠lia was the capital since the 1960s, so that's correct. \n",
      "\n",
      "Alternatively, I could use countries with more distinct capitals, like Canada (Ottawa), but I think France, Japan, and Brazil are all good examples. Let me double-check the capitals to be sure. Yep, Paris for France, Tokyo for Japan, and Bras√≠lia for Brazil. That should be accurate. I'll present them in a simple list format as requested.\n",
      "\n",
      "[2025-05-04 18:40:11] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 46, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-04 18:40:12] Decode batch. #running-req: 1, #token: 152, token usage: 0.00, gen throughput (token/s): 77.00, #queue-req: 0\n",
      "[2025-05-04 18:40:12] Decode batch. #running-req: 1, #token: 192, token usage: 0.00, gen throughput (token/s): 81.14, #queue-req: 0\n",
      "[2025-05-04 18:40:13] Decode batch. #running-req: 1, #token: 232, token usage: 0.00, gen throughput (token/s): 80.86, #queue-req: 0\n",
      "[2025-05-04 18:40:13] Decode batch. #running-req: 1, #token: 272, token usage: 0.00, gen throughput (token/s): 80.52, #queue-req: 0\n",
      "[2025-05-04 18:40:14] Decode batch. #running-req: 1, #token: 312, token usage: 0.00, gen throughput (token/s): 80.16, #queue-req: 0\n",
      "[2025-05-04 18:40:14] Decode batch. #running-req: 1, #token: 352, token usage: 0.00, gen throughput (token/s): 79.95, #queue-req: 0\n",
      "[2025-05-04 18:40:15] Decode batch. #running-req: 1, #token: 392, token usage: 0.00, gen throughput (token/s): 79.58, #queue-req: 0\n",
      "[2025-05-04 18:40:15] Decode batch. #running-req: 1, #token: 432, token usage: 0.00, gen throughput (token/s): 79.45, #queue-req: 0\n",
      "[2025-05-04 18:40:16] Decode batch. #running-req: 1, #token: 472, token usage: 0.00, gen throughput (token/s): 79.42, #queue-req: 0\n",
      "[2025-05-04 18:40:16] Decode batch. #running-req: 1, #token: 512, token usage: 0.00, gen throughput (token/s): 79.41, #queue-req: 0\n",
      "[2025-05-04 18:40:17] Decode batch. #running-req: 1, #token: 552, token usage: 0.00, gen throughput (token/s): 79.34, #queue-req: 0\n",
      "[2025-05-04 18:40:17] INFO:     127.0.0.1:54504 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "second_answer:\n",
      "Here‚Äôs another list of three countries and their capitals:  \n",
      "\n",
      "1. **Italy** - **Rome**  \n",
      "2. **Canada** - **Ottawa**  \n",
      "3. **South Africa** - **Pretoria**  \n",
      "\n",
      "Let me know if you‚Äôd like more examples! üòä\n",
      "\n",
      "\n",
      "second_answer_reasoning_content:\n",
      "Okay, the user asked for another list of three countries and their capitals. They previously got France, Japan, and Brazil. Now they want a different set. Let me think of some other countries.\n",
      "\n",
      "First, I need to make sure the countries are different from the first list. France, Japan, Brazil are already there. Maybe include some from different regions. Let's see... \n",
      "\n",
      "How about Italy? Its capital is Rome. That's a good one. Then maybe Canada, with Ottawa. That's in North America. Then a country from Africa, like South Africa with Pretoria. Wait, Pretoria is the capital, but I should double-check. Yes, South Africa's capital is Pretoria. \n",
      "\n",
      "Alternatively, maybe use another country. Let me confirm the capitals. Italy - Rome, Canada - Ottawa, South Africa - Pretoria. That should work. Let me make sure there's no mistake here. Rome is correct for Italy. Ottawa for Canada is right. Pretoria is the capital of South Africa. \n",
      "\n",
      "Alternatively, maybe include a country from Europe, like Germany. But then the user might get a repeat of Europe. Wait, the first list had France (Europe), Japan (Asia), Brazil (South America). So maybe the new list should have countries from different regions. \n",
      "\n",
      "So Italy is Europe, Canada is North America, South Africa is Africa. That covers three continents. That seems good. Alternatively, maybe include a country from Oceania, like Australia. But the user might not mind. Let me check. \n",
      "\n",
      "Alternatively, maybe use Canada, Australia, and India. But India's capital is New Delhi. But then maybe that's another set. But the user might want different regions. \n",
      "\n",
      "Wait, the first list had three different continents. So the second list should also have different ones. So Italy (Europe), Canada (North America), South Africa (Africa). That's a good mix. \n",
      "\n",
      "I think that's a solid list. Let me present them clearly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def multi_turn_qa(s):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(\"Please give me a list of 3 countries and their capitals.\")\n",
    "    s += assistant(separate_reasoning(gen(\"first_answer\", max_tokens=512), model_type=\"qwen3\"))\n",
    "    s += user(\"Please give me another list of 3 countries and their capitals.\")\n",
    "    s += assistant(separate_reasoning(gen(\"second_answer\", max_tokens=512), model_type=\"qwen3\"))\n",
    "    return s\n",
    "\n",
    "\n",
    "reasoning_state = multi_turn_qa()\n",
    "print_highlight(f\"\\n\\nfirst_answer:\\n{reasoning_state['first_answer']}\")\n",
    "print_highlight(f\"\\n\\nfirst_answer_reasoning_content:\\n{reasoning_state['first_answer_reasoning_content']}\")\n",
    "print_highlight(f\"\\n\\nsecond_answer:\\n{reasoning_state['second_answer']}\")\n",
    "print_highlight(f\"\\n\\nsecond_answer_reasoning_content:\\n{reasoning_state['second_answer_reasoning_content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using No thinking as Qwen 3's advanced feature \n",
    "\n",
    "sglang separate_reasoning is particularly useful when combined with Qwen 3's advanced feature.\n",
    "\n",
    "[Qwen 3's advanced usages](https://qwenlm.github.io/blog/qwen3/#advanced-usages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:35:04] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 26, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer_reasoning_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m reasoning_state = basic_qa(\u001b[33m\"\u001b[39m\u001b[33mList 3 countries and their capitals. /no_think\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReasoning Content:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[43mreasoning_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer_reasoning_content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mContent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, reasoning_state[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sglang/python/sglang/lang/interpreter.py:986\u001b[39m, in \u001b[36mProgramState.__getitem__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sglang/python/sglang/lang/interpreter.py:971\u001b[39m, in \u001b[36mProgramState.get_var\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_var\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sglang/python/sglang/lang/interpreter.py:333\u001b[39m, in \u001b[36mStreamExecutor.get_var\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.variable_event:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m.variable_event[name].wait()\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'answer_reasoning_content'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 18:35:04] INFO:     127.0.0.1:36614 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "reasoning_state = basic_qa(\"List 3 countries and their capitals. /no_think\")\n",
    "print(\"Reasoning Content:\\n\", reasoning_state[\"answer_reasoning_content\"])\n",
    "print(\"Content:\\n\", reasoning_state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def regular_expression_gen(s):\n",
    "    s += user(\"What is the IP address of the Google DNS servers? just provide the answer\")\n",
    "    s += separate_reasoning(\n",
    "        assistant(\n",
    "            gen(\n",
    "                \"answer\",\n",
    "                temperature=0,\n",
    "                regex=r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?).){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\",\n",
    "                max_tokens=512,\n",
    "            )\n",
    "        ), model_type=\"qwen3\"\n",
    "    )\n",
    "\n",
    "\n",
    "reasoning_state = regular_expression_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reasoning_state.messages()[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = r'''\n",
    "My name is Max.I am trying to talk to someone or something. Specifically, I want to \"hey fish\". I am probably using or involving this object in the interaction: Fresh-Caught Namazu Fish (MageMart is proud to work with aquariums all over the world to sustainably reduce the number of earthquakes across the globe.)\n",
    "   \n",
    "   Did I mention talking to as specific person, or am I talking to an object? Which is the more likely?\n",
    "   Do NOT give the final answer yet, but consider all the options and explain your logic in a single concise, brief, insightful sentence. Limit output to 40 words. Do not output line breaks.\n",
    "'''\n",
    "system_prompt = '''\n",
    "You are a helpful assistant. Respond with either Person or Object. Do not include any extra details or explanations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def my_chat(s):\n",
    "    global system_prompt, user_prompt\n",
    "    s += system(system_prompt)\n",
    "    s += user(user_prompt)\n",
    "    s += separate_reasoning(\n",
    "        assistant(\n",
    "            gen(\n",
    "                \"answer\",\n",
    "                temperature=0.3,\n",
    "                regex=r'(Yes|No)',\n",
    "                max_tokens=512,\n",
    "            )\n",
    "        ), model_type=\"qwen3\"\n",
    "    )\n",
    "\n",
    "\n",
    "reasoning_state = my_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(reasoning_state.messages()[-1]['reasoning_content'])\n",
    "pprint(reasoning_state.messages()[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
