{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server with a reasoning model (Qwen 3.5-4B) and reasoning parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/sglang/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:10:15] server_args=ServerArgs(model_path='Qwen/Qwen3-4B', tokenizer_path='Qwen/Qwen3-4B', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen3-4B', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=32857, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=569616803, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='qwen3', dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\n",
      "[2025-05-05 17:10:21] Attention backend not set. Use flashinfer backend by default.\n",
      "[2025-05-05 17:10:21] Init torch distributed begin.\n",
      "[2025-05-05 17:10:21] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-05-05 17:10:21] Load weight begin. avail mem=43.89 GB\n",
      "[2025-05-05 17:10:22] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  4.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.49it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.69it/s]\n",
      "\n",
      "[2025-05-05 17:10:23] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=36.25 GB, mem usage=7.63 GB.\n",
      "[2025-05-05 17:10:23] KV Cache is allocated. #tokens: 225647, K size: 15.49 GB, V size: 15.49 GB\n",
      "[2025-05-05 17:10:23] Memory pool end. avail mem=4.71 GB\n",
      "2025-05-05 17:10:23,807 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "[2025-05-05 17:10:23] Capture cuda graph begin. This can take up to several minutes. avail mem=4.09 GB\n",
      "[2025-05-05 17:10:23] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]\n",
      "Capturing batches (avail_mem=4.06 GB):   0%|          | 0/23 [00:00<?, ?it/s]2025-05-05 17:10:24,272 - INFO - flashinfer.jit: Loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n",
      "2025-05-05 17:10:24,294 - INFO - flashinfer.jit: Finished loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n",
      "Capturing batches (avail_mem=2.68 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:06<00:00,  3.65it/s]\n",
      "[2025-05-05 17:10:30] Capture cuda graph end. Time elapsed: 6.35 s. mem usage=1.41 GB. avail mem=2.67 GB.\n",
      "[2025-05-05 17:10:30] max_total_num_tokens=225647, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2821, context_len=40960\n",
      "[2025-05-05 17:10:31] INFO:     Started server process [1083689]\n",
      "[2025-05-05 17:10:31] INFO:     Waiting for application startup.\n",
      "[2025-05-05 17:10:31] INFO:     Application startup complete.\n",
      "[2025-05-05 17:10:31] INFO:     Uvicorn running on http://0.0.0.0:32857 (Press CTRL+C to quit)\n",
      "[2025-05-05 17:10:31] INFO:     127.0.0.1:39508 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-05-05 17:10:32] INFO:     127.0.0.1:39522 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-05-05 17:10:32] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "2025-05-05 17:10:32,677 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-05-05 17:10:32,700 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "[2025-05-05 17:10:33] INFO:     127.0.0.1:39538 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-05-05 17:10:33] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.\n",
      "                    \n",
      "Server started on http://localhost:32857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sglang import separate_reasoning, assistant_begin, assistant_end\n",
    "from sglang import assistant, function, gen, system, user\n",
    "from sglang import image\n",
    "from sglang import RuntimeEndpoint, set_default_backend\n",
    "from sglang.srt.utils import load_image\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "from sglang.utils import print_highlight, terminate_process, wait_for_server\n",
    "\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen3-4B --reasoning-parser qwen3 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")\n",
    "print(f\"Server started on http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default backend. Note: you can set chat_template_name in RontimeEndpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:10:36] INFO:     127.0.0.1:43444 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "set_default_backend(RuntimeEndpoint(f\"http://localhost:{port}\", chat_template_name=\"qwen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a basic question-answering task. And see how the reasoning content is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:10:36] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-05 17:10:36] Decode batch. #running-req: 1, #token: 64, token usage: 0.00, gen throughput (token/s): 6.34, #queue-req: 0\n",
      "[2025-05-05 17:10:37] Decode batch. #running-req: 1, #token: 104, token usage: 0.00, gen throughput (token/s): 82.06, #queue-req: 0\n",
      "[2025-05-05 17:10:37] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, gen throughput (token/s): 81.59, #queue-req: 0\n",
      "[2025-05-05 17:10:38] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, gen throughput (token/s): 81.15, #queue-req: 0\n",
      "[2025-05-05 17:10:38] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, gen throughput (token/s): 80.91, #queue-req: 0\n",
      "[2025-05-05 17:10:39] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, gen throughput (token/s): 80.59, #queue-req: 0\n",
      "[2025-05-05 17:10:39] Decode batch. #running-req: 1, #token: 304, token usage: 0.00, gen throughput (token/s): 80.22, #queue-req: 0\n",
      "[2025-05-05 17:10:40] Decode batch. #running-req: 1, #token: 344, token usage: 0.00, gen throughput (token/s): 79.99, #queue-req: 0\n",
      "[2025-05-05 17:10:40] INFO:     127.0.0.1:43452 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "<think>\n",
      "Okay, the user is asking for three countries and their capitals. Let me think. I need to make sure the country and capital pairs are correct.\n",
      "\n",
      "First, I'll start with a well-known country. France is a good example. Its capital is Paris. That's definitely correct. \n",
      "\n",
      "Next, maybe a country in Asia. Japan comes to mind. The capital there is Tokyo. Yep, that's right. \n",
      "\n",
      "Then, I should pick another country. How about Germany? Its capital is Berlin. Wait, is that correct? I think so, but I should double-check. Yes, Berlin is the capital of Germany. \n",
      "\n",
      "Alternatively, maybe pick a country from Africa. Egypt's capital is Cairo. That's another solid example. \n",
      "\n",
      "Wait, the user just needs three. So France, Japan, and Germany would work. Or maybe France, Japan, and Egypt. Let me confirm each one again. \n",
      "\n",
      "Paris is the capital of France. Tokyo is the capital of Japan. Berlin is the capital of Germany. All correct. \n",
      "\n",
      "Alternatively, maybe use more well-known capitals. Like Canada's capital is Ottawa, but that's another country. Let me stick with the first three I thought of. France, Japan, Germany. \n",
      "\n",
      "Alternatively, maybe include a country from South America. Brazil's capital is BrasÃ­lia. But maybe the user is looking for more commonly known capitals. \n",
      "\n",
      "I think the three I have are safe. Let me present them in order. France - Paris, Japan - Tokyo, Germany - Berlin. That should be accurate.\n",
      "</think>\n",
      "\n",
      "1. **France** - Paris  \n",
      "2. **Japan** - Tokyo  \n",
      "3. **Germany** - Berlin\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def basic_qa(s, question):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(question)\n",
    "    s += assistant_begin()\n",
    "    s += gen(\"answer\", max_tokens=512)\n",
    "    s += assistant_end()\n",
    "\n",
    "state = basic_qa(\"List 3 countries and their capitals.\")\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `separate_reasoning`, you can move the reasoning content to `{param_name}_reasoning_content` in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['answer', 'answer_reasoning_content'])\n",
      "[2025-05-05 17:10:40] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "2025-05-05 17:10:40,722 - INFO - flashinfer.jit: Loading JIT ops: cascade\n",
      "2025-05-05 17:10:40,743 - INFO - flashinfer.jit: Finished loading JIT ops: cascade\n",
      "[2025-05-05 17:10:40] Decode batch. #running-req: 1, #token: 44, token usage: 0.00, gen throughput (token/s): 73.25, #queue-req: 0\n",
      "[2025-05-05 17:10:41] Decode batch. #running-req: 1, #token: 84, token usage: 0.00, gen throughput (token/s): 82.31, #queue-req: 0\n",
      "[2025-05-05 17:10:41] Decode batch. #running-req: 1, #token: 124, token usage: 0.00, gen throughput (token/s): 81.89, #queue-req: 0\n",
      "[2025-05-05 17:10:42] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, gen throughput (token/s): 81.34, #queue-req: 0\n",
      "[2025-05-05 17:10:42] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, gen throughput (token/s): 81.05, #queue-req: 0\n",
      "[2025-05-05 17:10:43] Decode batch. #running-req: 1, #token: 244, token usage: 0.00, gen throughput (token/s): 80.80, #queue-req: 0\n",
      "[2025-05-05 17:10:43] Decode batch. #running-req: 1, #token: 284, token usage: 0.00, gen throughput (token/s): 80.42, #queue-req: 0\n",
      "[2025-05-05 17:10:44] Decode batch. #running-req: 1, #token: 324, token usage: 0.00, gen throughput (token/s): 80.08, #queue-req: 0\n",
      "[2025-05-05 17:10:44] Decode batch. #running-req: 1, #token: 364, token usage: 0.00, gen throughput (token/s): 79.77, #queue-req: 0\n",
      "[2025-05-05 17:10:44] INFO:     127.0.0.1:43454 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "Separated Reasoning Content:\n",
      "Okay, the user is asking for three countries and their capitals. Let me start by recalling some basic information. First, I need to make sure I know the correct capitals for different countries. Let me think of a few well-known ones.\n",
      "\n",
      "France's capital is Paris. That's straightforward. Then, Canada's capital is Ottawa. I remember that from geography class. What about another one? Maybe Brazil? Its capital is BrasÃ­lia. Wait, I should double-check that. Yes, BrasÃ­lia is correct. Alternatively, I could use Germany, which has Berlin as its capital. But I think using Brazil's capital might be a good example because sometimes people might confuse it with SÃ£o Paulo, which is a major city but not the capital.\n",
      "\n",
      "Wait, the user didn't specify any particular region or continent, so I should pick countries from different regions to show diversity. Let me confirm: France is in Europe, Canada in North America, and Brazil in South America. That covers three different continents. Alternatively, maybe include one from Asia? But the user asked for three, so three is manageable. Let me check again: France (Paris), Canada (Ottawa), Brazil (BrasÃ­lia). Yeah, those are correct. I think that's a solid answer. Make sure the spelling is right. Paris, Ottawa, BrasÃ­lia. Yeah, that's correct. I don't think I made any mistakes here.\n",
      "\n",
      "\n",
      "\n",
      "Content:\n",
      "Here are three countries and their capitals:  \n",
      "1. **France** â€“ Paris  \n",
      "2. **Canada** â€“ Ottawa  \n",
      "3. **Brazil** â€“ BrasÃ­lia  \n",
      "\n",
      "Let me know if you'd like more examples! ðŸ˜Š\n",
      "\n",
      "\n",
      "Messages:\n",
      "{'role': 'assistant', 'content': \"Here are three countries and their capitals:  \\n1. **France** â€“ Paris  \\n2. **Canada** â€“ Ottawa  \\n3. **Brazil** â€“ BrasÃ­lia  \\n\\nLet me know if you'd like more examples! ðŸ˜Š\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@function\n",
    "def basic_qa_separate_reasoning(s, question):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(question)\n",
    "    s += assistant_begin()\n",
    "    s += separate_reasoning(\n",
    "        gen(\"answer\", max_tokens=512),\n",
    "        model_type=\"qwen3\"\n",
    "    )\n",
    "    s += assistant_end()\n",
    "\n",
    "reasoning_state = basic_qa_separate_reasoning(\"List 3 countries and their capitals.\")\n",
    "print_highlight(reasoning_state.stream_executor.variable_event.keys())\n",
    "print_highlight(f\"\\nSeparated Reasoning Content:\\n{reasoning_state[\"answer_reasoning_content\"]}\")\n",
    "print_highlight(f\"\\n\\nContent:\\n{reasoning_state[\"answer\"]}\")\n",
    "print_highlight(f\"\\n\\nMessages:\\n{reasoning_state.messages()[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`separate_reasoning` can also be used in multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:10:44] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 18, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-05 17:10:45] Decode batch. #running-req: 1, #token: 74, token usage: 0.00, gen throughput (token/s): 77.84, #queue-req: 0\n",
      "[2025-05-05 17:10:45] Decode batch. #running-req: 1, #token: 114, token usage: 0.00, gen throughput (token/s): 81.93, #queue-req: 0\n",
      "[2025-05-05 17:10:46] Decode batch. #running-req: 1, #token: 154, token usage: 0.00, gen throughput (token/s): 81.38, #queue-req: 0\n",
      "[2025-05-05 17:10:46] Decode batch. #running-req: 1, #token: 194, token usage: 0.00, gen throughput (token/s): 81.06, #queue-req: 0\n",
      "[2025-05-05 17:10:47] Decode batch. #running-req: 1, #token: 234, token usage: 0.00, gen throughput (token/s): 80.80, #queue-req: 0\n",
      "[2025-05-05 17:10:47] Decode batch. #running-req: 1, #token: 274, token usage: 0.00, gen throughput (token/s): 80.44, #queue-req: 0\n",
      "[2025-05-05 17:10:47] INFO:     127.0.0.1:52600 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "first_answer:\n",
      "Hereâ€™s a list of three countries and their capitals:\n",
      "\n",
      "1. **France** â€“ **Paris**  \n",
      "2. **Brazil** â€“ **BrasÃ­lia**  \n",
      "3. **Germany** â€“ **Berlin**  \n",
      "\n",
      "Let me know if you'd like more examples! ðŸ˜Š\n",
      "\n",
      "\n",
      "first_answer_reasoning_content:\n",
      "Okay, the user is asking for a list of 3 countries and their capitals. Let me start by recalling some well-known countries and their capitals. First, I should make sure the information is correct. France's capital is Paris, that's definitely right. Then, Brazil's capital is BrasÃ­lia. Wait, sometimes people might think it's Rio de Janeiro, but no, BrasÃ­lia is the capital. For the third one, maybe Germany? Its capital is Berlin. Alternatively, I could pick another country like Canada with Ottawa, or Italy with Rome. But the user just needs three, so France, Brazil, and Germany should be fine. Let me double-check each one to avoid mistakes. Paris is correct for France. BrasÃ­lia is the capital of Brazil. Berlin is the capital of Germany. Yep, that's three countries and their capitals. I think that's all the user needs. No need to complicate it further.\n",
      "\n",
      "[2025-05-05 17:10:47] Prefill batch. #new-seq: 1, #new-token: 78, #cached-token: 36, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-05 17:10:48] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, gen throughput (token/s): 76.42, #queue-req: 0\n",
      "[2025-05-05 17:10:48] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, gen throughput (token/s): 81.12, #queue-req: 0\n",
      "[2025-05-05 17:10:49] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, gen throughput (token/s): 80.91, #queue-req: 0\n",
      "[2025-05-05 17:10:49] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, gen throughput (token/s): 80.57, #queue-req: 0\n",
      "[2025-05-05 17:10:50] Decode batch. #running-req: 1, #token: 304, token usage: 0.00, gen throughput (token/s): 80.18, #queue-req: 0\n",
      "[2025-05-05 17:10:50] Decode batch. #running-req: 1, #token: 344, token usage: 0.00, gen throughput (token/s): 79.92, #queue-req: 0\n",
      "[2025-05-05 17:10:51] Decode batch. #running-req: 1, #token: 384, token usage: 0.00, gen throughput (token/s): 79.61, #queue-req: 0\n",
      "[2025-05-05 17:10:51] INFO:     127.0.0.1:52606 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "second_answer:\n",
      "Hereâ€™s another list of three countries and their capitals:  \n",
      "\n",
      "1. **Italy** â€“ **Rome**  \n",
      "2. **Canada** â€“ **Ottawa**  \n",
      "3. **Japan** â€“ **Tokyo**  \n",
      "\n",
      "Let me know if you'd like more examples! ðŸ˜Š\n",
      "\n",
      "\n",
      "second_answer_reasoning_content:\n",
      "Okay, the user just asked for another list of three countries and their capitals. Let me check the previous conversation. They first asked for a list, and I provided France, Brazil, and Germany. Now, they want another list, so I need to give different countries.\n",
      "\n",
      "I should make sure the countries are not the same as the first list. Let me think of some other countries. Maybe start with Italy, which is in Europe. Its capital is Rome. Then, Canada is in North America, and its capital is Ottawa. Lastly, Japan is in East Asia, with Tokyo as the capital. \n",
      "\n",
      "Wait, are those all correct? Italy's capital is indeed Rome. Canada's capital is Ottawa, not Toronto. Japan's capital is Tokyo. Yep, that's right. So the list would be Italy (Rome), Canada (Ottawa), Japan (Tokyo). That should work. I should present them clearly, maybe in bullet points again. Also, the user might be testing if I remember the capitals correctly, so accuracy is key here. Let me confirm each one once more to be sure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def multi_turn_qa(s):\n",
    "    s += system(f\"You are a helpful assistant than can answer questions.\")\n",
    "    s += user(\"Please give me a list of 3 countries and their capitals.\")\n",
    "    s += assistant(separate_reasoning(gen(\"first_answer\", max_tokens=512), model_type=\"qwen3\"))\n",
    "    s += user(\"Please give me another list of 3 countries and their capitals.\")\n",
    "    s += assistant(separate_reasoning(gen(\"second_answer\", max_tokens=512), model_type=\"qwen3\"))\n",
    "    return s\n",
    "\n",
    "\n",
    "reasoning_state = multi_turn_qa()\n",
    "print_highlight(f\"\\n\\nfirst_answer:\\n{reasoning_state['first_answer']}\")\n",
    "print_highlight(f\"\\n\\nfirst_answer_reasoning_content:\\n{reasoning_state['first_answer_reasoning_content']}\")\n",
    "print_highlight(f\"\\n\\nsecond_answer:\\n{reasoning_state['second_answer']}\")\n",
    "print_highlight(f\"\\n\\nsecond_answer_reasoning_content:\\n{reasoning_state['second_answer_reasoning_content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using No thinking as Qwen 3's advanced feature \n",
    "\n",
    "sglang separate_reasoning is particularly useful when combined with Qwen 3's advanced feature.\n",
    "\n",
    "[Qwen 3's advanced usages](https://qwenlm.github.io/blog/qwen3/#advanced-usages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:10:51] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 26, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-05 17:10:51] Decode batch. #running-req: 1, #token: 57, token usage: 0.00, gen throughput (token/s): 76.85, #queue-req: 0\n",
      "[2025-05-05 17:10:51] INFO:     127.0.0.1:52612 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "Reasoning Content:\n",
      " \n",
      "Content:\n",
      " 1. **France** - Paris  \n",
      "2. **Japan** - Tokyo  \n",
      "3. **Canada** - Ottawa\n"
     ]
    }
   ],
   "source": [
    "reasoning_state = basic_qa_separate_reasoning(\"List 3 countries and their capitals. /no_think\")\n",
    "print(\"Reasoning Content:\\n\", reasoning_state[\"answer_reasoning_content\"])\n",
    "print(\"Content:\\n\", reasoning_state[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`separate_reasoning` can also be used in regular expression generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 17:11:17] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-05 17:11:17] Decode batch. #running-req: 1, #token: 68, token usage: 0.00, gen throughput (token/s): 1.54, #queue-req: 0\n",
      "[2025-05-05 17:11:18] Decode batch. #running-req: 1, #token: 108, token usage: 0.00, gen throughput (token/s): 83.08, #queue-req: 0\n",
      "[2025-05-05 17:11:18] Decode batch. #running-req: 1, #token: 148, token usage: 0.00, gen throughput (token/s): 82.53, #queue-req: 0\n",
      "[2025-05-05 17:11:19] Decode batch. #running-req: 1, #token: 188, token usage: 0.00, gen throughput (token/s): 82.11, #queue-req: 0\n",
      "[2025-05-05 17:11:19] Decode batch. #running-req: 1, #token: 228, token usage: 0.00, gen throughput (token/s): 81.86, #queue-req: 0\n",
      "[2025-05-05 17:11:20] Decode batch. #running-req: 1, #token: 268, token usage: 0.00, gen throughput (token/s): 81.53, #queue-req: 0\n",
      "[2025-05-05 17:11:20] Decode batch. #running-req: 1, #token: 308, token usage: 0.00, gen throughput (token/s): 81.14, #queue-req: 0\n",
      "[2025-05-05 17:11:21] Decode batch. #running-req: 1, #token: 348, token usage: 0.00, gen throughput (token/s): 80.88, #queue-req: 0\n",
      "[2025-05-05 17:11:21] INFO:     127.0.0.1:37928 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "@function\n",
    "def regular_expression_gen(s):\n",
    "    s += user(\"What is the IP address of the Google DNS servers? just provide the answer\")\n",
    "    s += assistant(\n",
    "        separate_reasoning(\n",
    "            gen(\n",
    "                \"answer\",\n",
    "                temperature=0,\n",
    "                regex=r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?).){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\",\n",
    "                max_tokens=512,\n",
    "            )\n",
    "        ,model_type=\"qwen3\"), \n",
    "    )\n",
    "\n",
    "\n",
    "reasoning_state = regular_expression_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "2023-10-05\n",
      "\n",
      "\n",
      "Reasoning Content:\n",
      "Okay, the user is asking for the IP addresses of Google's DNS servers. Let me recall what I know about DNS servers. Google provides two public DNS servers, right? They're commonly used for their reliability and speed.\n",
      "\n",
      "I think the primary one is 8.8.8.8. Wait, isn't there another one? Oh yeah, 8.8.4.4. Those are the two main ones. Let me make sure I'm not mixing them up with other providers. For example, Cloudflare uses 1.1.1.1 and 1.0.0.1. But Google's are definitely 8.8.8.8 and 8.8.4.4. \n",
      "\n",
      "I should check if there are any other IP addresses, but I don't think so. They have two main ones. The user might be looking to set up their DNS settings, so providing both is important. Also, maybe mention that they're both in the same range, which is 8.8.0.0/14. But the user just asked for the IP addresses, so maybe just list them. \n",
      "\n",
      "Wait, the user said \"just provide the answer,\" so maybe they don't need extra info. But to be thorough, I should confirm that those are the correct ones. Let me think if there's any chance of confusion. No, 8.8.8.8 is the primary, and 8.8.4.4 is the secondary. Yeah, that's right. So the answer is those two IPs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_highlight(f\"Answer:\\n{reasoning_state['answer']}\")\n",
    "print_highlight(f\"\\n\\nReasoning Content:\\n{reasoning_state['answer_reasoning_content']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
