PCG Bugs (need manual investigation/fix)
=========================================

Test                                          Failure Summary
--------------------------------------------------------------------------------------------------------------
scheduler/test_retract_decode.py              CUDA device-side assert (vectorized_gather_kernel index out of bounds) during decode retraction + re-prefill under piecewise CUDA graph
core/test_gpt_oss_1gpu.py                                     2/10; CUDA device-side assert under PCG; both bf16 and mxfp4 models affected; garbage output after CUDA error
spec/eagle/test_eagle_infer_b.py                              [NOT PCG] 7/10; vectorized_gather_kernel index OOB in test_radix_attention; PCG auto-disabled for EAGLE (disable_piecewise_cuda_graph=True on all 6 servers); EAGLE + radix attention bug, not PCG
lora/test_lora_tp.py (2-GPU)                                  0/10; LoRA servers correctly auto-disable PCG; crash is in the no-LoRA baseline server (run_lora_test_one_by_one launches 2nd SRTRunner without lora_paths for comparison); PCG + TP=2 on Llama-2-7b-hf crashes with CUDA illegal memory access in pynccl.outplace_all_reduce during PCG warmup
lora/test_lora_backend.py                                      0/10; FusedAddRMSNorm illegal memory access on Llama-2-7b-hf baseline server (no LoRA) during PCG warmup; root cause: patch_model skips forward_native when compiler="eager", so CUDA kernel called inside torch.compile; same pattern as test_lora_tp
openai_server/validation/test_large_max_new_tokens.py         [PASSED] rerun 4/5 passed; baseline also has same race condition; not PCG
eval/test_text_models_gsm8k_eval.py (2-GPU)                   [PASSED] bmm_fp8 torch.compile issue fixed
models/test_generation_models.py                              [PASSED] CI only runs 2 models (Llama-3.1-8B, gemma-2-2b) which pass with PCG; Qwen2-1.5B decode logprobs 0.0606>0.06 only in local test_all_models; not CI blocker
quant/test_torchao.py                                        9/10; vectorized_gather_kernel index OOB + CUDA device-side assert; same pattern as test_retract_decode; PCG active; flaky

Resolved / Not PCG-related (kept for reference)
================================================
tokenizer/test_skip_tokenizer_init.py         [RESOLVED] Was 9/10 flaky; rerun 10/10 passed; environment issue
openai_server/features/test_enable_thinking.py               [RESOLVED] Was 0/10; rerun 10/10 passed; environment issue
lora/test_lora_openai_compatible.py                           [RESOLVED] Was 0/10; rerun 10/10 passed; environment issue
quant/test_autoround.py                                      [FIXED] rerun 10/10 passed; awq_dequantize fake tensor impl wrong 6-arg -> 3-arg signature
models/test_embedding_models.py                                [FIXED] rerun 10/10 passed; JIT kernel warmup before torch.compile (commit a2d81bf)
scheduler/test_abort.py                                        [RESOLVED] rerun 10/10 passed; was 9/10 flaky
attention/test_triton_sliding_window.py                        [RESOLVED] rerun 10/10 passed; was 8/10 flaky
lora/test_multi_lora_backend.py                               [PASSED] Not PCG-related; LoRA auto-disables PCG
quant/test_awq.py                                             [PASSED] Not PCG-related; VLM model auto-disables PCG; flaky MMLU score
spec/eagle/test_eagle_infer_a.py                             [NOT PCG] EAGLE3 context_length mismatch; PCG auto-disabled for speculative decoding
spec/eagle/test_eagle3_basic.py                              [NOT PCG] OOM during EAGLE3 draft model loading; PCG auto-disabled
spec/eagle/test_eagle_infer_beta.py                          [NOT PCG] OOM during EAGLE draft model loading; PCG auto-disabled
attention/test_fa3.py                                        [NOT PCG] 404 on private CI model + EAGLE3 context_length mismatch
openai_server/features/test_openai_server_hidden_states.py   [NOT PCG] EAGLE3 context_length mismatch; PCG auto-disabled
vlm/test_vision_chunked_prefill.py                            [NOT PCG] ROUGE-L flaky; PCG auto-disabled for VLM
lora/test_lora_qwen3.py                                       [NOT PCG] Test too slow + timeout; cuda graph disabled (torch native attn backend)
models/test_vlm_models.py                                     [NOT PCG] Gated HF model (openbmb/MiniCPM-V-2_6); 403 access denied; PCG not active (VLM auto-disabled)
perf/test_bench_serving_1gpu_part2.py                        [NOT PCG] 9/10; flaky 503 health check race during server startup; PCG compiled successfully
quant/test_w8a8_quantization.py                              [NOT PCG] 9/10; borderline accuracy assertion (0.69 not > 0.69 threshold); flaky
tokenizer/test_multi_tokenizer.py                            [NOT PCG] 9/10; server hung during warmup HTTP timeout; flaky
scheduler/test_routing_key_scheduling.py                     [NOT PCG] 2/10; no PCG activity detected; timing assertion flaky (sub-ms latency diffs)
attention/test_hybrid_attn_backend.py                        [NOT PCG] 0/10; private CI model 404 (lmsys/sglang-ci-dsv3-test); first test class passed with PCG active
lora/test_lora_update.py                                     [NOT PCG] 0/10; test timeout at 1200s + leaked server process (PID 202958); infra/cleanup issue
perf/test_bench_serving_1gpu_part1.py                        [NOT PCG] 0/10; actually passed with PCG disabled; listed as failure due to OOM leak from prior test
vlm/test_vision_openai_server_a.py                           [NOT PCG] 0/10; PCG not active; multimodal data not transmitted to models; OpenAI API compatibility issue
