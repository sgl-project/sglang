name: PR Test (PD Router)

on:
  push:
    branches: [ main ]
    paths:
      - 'python/sglang/srt/disaggregation/**'
      - 'sgl-router/**'
      - '.github/workflows/pr-test-pd-router.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'python/sglang/srt/disaggregation/**'
      - 'sgl-router/**'
      - '.github/workflows/pr-test-pd-router.yml'
    types: [synchronize, labeled]
  workflow_dispatch:

concurrency:
  group: test-disaggregation-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  test-disaggregation:
    if: github.event_name != 'pull_request' || (contains(github.event.pull_request.labels.*.name, 'run-ci') && contains(github.event.pull_request.labels.*.name, 'router-benchmark'))
    runs-on: [h200]
    timeout-minutes: 45

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 10

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Setup Rust
      run: |
        bash scripts/ci/ci_install_rust.sh

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          sgl-router/target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('sgl-router/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('python/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Validate environment
      run: |
        echo "=== System Validation ==="
        nvidia-smi
        echo "GPU count: $(nvidia-smi -L | wc -l)"
        if [ $(nvidia-smi -L | wc -l) -lt 8 ]; then
          echo "Error: This test requires at least 8 GPUs"
          exit 1
        fi

        echo "=== GPU Process Check ==="
        # Fail fast if any GPU compute processes are active
        if command -v nvidia-smi >/dev/null 2>&1; then
          # Try to query compute apps first (preferred and concise)
          gpu_procs=$(nvidia-smi --query-compute-apps=pid,process_name,gpu_uuid --format=csv,noheader 2>/dev/null | sed '/^$/d' || true)

          # Fallback to detailed PIDS report if the query returns nothing but there might still be processes
          if [ -z "$gpu_procs" ]; then
            gpu_procs=$(nvidia-smi -q -d PIDS 2>/dev/null | awk '/Processes/{flag=1;next}/^$/{flag=0}flag' | sed '/^\s*Processes:/d' | sed '/^\s*$/d' || true)
          fi

          if [ -n "$gpu_procs" ]; then
            echo "Error: Found active GPU processes using the device(s):"
            echo "$gpu_procs"
            exit 1
          else
            echo "No active GPU compute processes detected."
          fi
        else
          echo "Error: nvidia-smi not found; skipping GPU process check."
          exit 1
        fi

        echo "=== RDMA Validation ==="
        if ! command -v ibv_devices >/dev/null 2>&1; then
          echo "Error: InfiniBand tools not found"
          exit 1
        fi

        # Check for active IB devices
        found_active_device=false
        for device in mlx5_{0..11}; do
            if ibv_devinfo $device >/dev/null 2>&1; then
                state=$(ibv_devinfo $device | grep "state:" | head -1 | awk '{print $2}')
                if [[ "$state" == "PORT_ACTIVE" ]]; then
                    echo "✓ Found active device: $device"
                    found_active_device=true
                    break
                fi
            fi
        done

        if [ "$found_active_device" = false ]; then
          echo "Error: No active IB devices found"
          echo "Available devices:"
          ibv_devices || true
          exit 1
        fi

        echo "=== Model Validation ==="
        if [ ! -d "/raid/models/meta-llama/Llama-3.1-8B-Instruct" ]; then
          echo "Error: Model not found"
          ls -la /raid/models/ || echo "No models directory"
          exit 1
        fi
        echo "✓ Model found"

    - name: Install SGLang dependencies
      run: |
        echo "Installing SGLang with all extras..."
        python3 -m pip --no-cache-dir install --upgrade pip
        python3 -m pip --no-cache-dir install torch==2.8.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu126
        python3 -m pip --no-cache-dir install -e "python[all]" --break-system-packages
        python3 -m pip --no-cache-dir install mooncake-transfer-engine==0.3.6.post1
        python3 -m pip --no-cache-dir install --force-reinstall genai-bench==0.0.2 pytest pytest-html

    - name: Build and install sgl-router
      run: |
        source "$HOME/.cargo/env"
        echo "Building sgl-router..."
        cd sgl-router
        cargo build && python3 -m build && pip install --force-reinstall dist/*.whl

    - name: Run PD router performance tests
      env:
        PYTHONUNBUFFERED: "1"
        PD_PERF_RUN: "1"
      run: |
        cd sgl-router
        pytest py_test/perf/test_pd_router_perf.py -m perf -vv -s --maxfail=1

    - name: Upload benchmark results
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: pd-router-perf-results
        path: sgl-router/pd_perf_benchmark_*/

  summarize-benchmarks:
    needs: test-disaggregation
    runs-on: ubuntu-latest
    if: success()

    steps:
    - name: Install jq
      run: sudo apt-get update && sudo apt-get install -y jq bc

    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: pd-router-perf-results

    - name: List downloaded contents
      run: |
        echo "Contents after download:"
        ls -la
        find . -name "pd_perf_benchmark_*" -type d
        echo "JSON files found:"
        find . -name "*.json" | head -10

    - name: Create benchmark summary
      run: |
        echo "=== DEBUG: Creating benchmark summary ==="
        echo "Available benchmark directories:"
        find . -name "pd_perf_benchmark_*" -type d || true
        echo "=========================================="

        echo "## PD Router Genai-Bench Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "🚀 **Perf benchmarks executed via pytest perf suite**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Policy | Status | TTFT (s) | E2E Latency (s) | Input Throughput (tok/s) | Output Throughput (tok/s) |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|--------|----------|-----------------|--------------------------|---------------------------|" >> $GITHUB_STEP_SUMMARY

        for policy in random round_robin cache_aware power_of_two; do
          pattern="pd_perf_benchmark_${policy}"
          result_folder=$(find . -maxdepth 5 -path "*${pattern}" -type d | head -1)

          echo "DEBUG: Policy ${policy} -> Found folder: ${result_folder:-'NOT FOUND'}"

          if [ -n "$result_folder" ] && [ -d "$result_folder" ]; then
            json_file=$(find "$result_folder" -name "*.json" -not -name "experiment_metadata.json" | head -1)

            if [ -n "$json_file" ] && [ -f "$json_file" ]; then
              ttft_mean=$(jq -r '.aggregated_metrics.stats.ttft.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              e2e_latency_mean=$(jq -r '.aggregated_metrics.stats.e2e_latency.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              input_throughput_mean=$(jq -r '.aggregated_metrics.stats.input_throughput.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              output_throughput_mean=$(jq -r '.aggregated_metrics.stats.output_throughput.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")

              if [ "$ttft_mean" != "N/A" ] && [ "$ttft_mean" != "null" ]; then
                ttft_display=$(printf "%.2f" "$ttft_mean" 2>/dev/null || echo "$ttft_mean")
              else
                ttft_display="N/A"
              fi

              if [ "$e2e_latency_mean" != "N/A" ] && [ "$e2e_latency_mean" != "null" ]; then
                e2e_display=$(printf "%.2f" "$e2e_latency_mean" 2>/dev/null || echo "$e2e_latency_mean")
              else
                e2e_display="N/A"
              fi

              if [ "$input_throughput_mean" != "N/A" ] && [ "$input_throughput_mean" != "null" ]; then
                input_display=$(printf "%.0f" "$input_throughput_mean" 2>/dev/null || echo "$input_throughput_mean")
              else
                input_display="N/A"
              fi

              if [ "$output_throughput_mean" != "N/A" ] && [ "$output_throughput_mean" != "null" ]; then
                output_display=$(printf "%.0f" "$output_throughput_mean" 2>/dev/null || echo "$output_throughput_mean")
              else
                output_display="N/A"
              fi

              echo "| ${policy} | ✅ Success | $ttft_display | $e2e_display | $input_display | $output_display |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| ${policy} | ❌ No Data | N/A | N/A | N/A | N/A |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| ${policy} | ❌ Failed | N/A | N/A | N/A | N/A |" >> $GITHUB_STEP_SUMMARY
          fi
        done

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 📊 Performance Validation" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Thresholds:** TTFT ≤ 4.7s | E2E Latency ≤ 35.0s | Input Throughput ≥ 10,000 tok/s | Output Throughput ≥ 68 tok/s" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        validation_summary=""
        for policy in random round_robin cache_aware power_of_two; do
          pattern="pd_perf_benchmark_${policy}"
          result_folder=$(find . -maxdepth 5 -path "*${pattern}" -type d | head -1)

          if [ -n "$result_folder" ] && [ -d "$result_folder" ]; then
            json_file=$(find "$result_folder" -name "*.json" -not -name "experiment_metadata.json" | head -1)
            if [ -n "$json_file" ] && [ -f "$json_file" ]; then
              ttft=$(jq -r '.aggregated_metrics.stats.ttft.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              e2e_latency=$(jq -r '.aggregated_metrics.stats.e2e_latency.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              input_throughput=$(jq -r '.aggregated_metrics.stats.input_throughput.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")
              output_throughput=$(jq -r '.aggregated_metrics.stats.output_throughput.mean // "N/A"' "$json_file" 2>/dev/null || echo "N/A")

              status="✅"
              if [ "$ttft" != "N/A" ] && [ "$ttft" != "null" ] && (( $(echo "$ttft > 4.7" | bc -l 2>/dev/null || echo "0") )); then
                status="❌"
              fi
              if [ "$e2e_latency" != "N/A" ] && [ "$e2e_latency" != "null" ] && (( $(echo "$e2e_latency > 35.0" | bc -l 2>/dev/null || echo "0") )); then
                status="❌"
              fi
              if [ "$input_throughput" != "N/A" ] && [ "$input_throughput" != "null" ] && (( $(echo "$input_throughput < 10000" | bc -l 2>/dev/null || echo "0") )); then
                status="❌"
              fi
              if [ "$output_throughput" != "N/A" ] && [ "$output_throughput" != "null" ] && (( $(echo "$output_throughput < 68" | bc -l 2>/dev/null || echo "0") )); then
                status="❌"
              fi

              validation_summary="${validation_summary}- **${policy}**: ${status}\n"
            else
              validation_summary="${validation_summary}- **${policy}**: ❌ No data\n"
            fi
          else
            validation_summary="${validation_summary}- **${policy}**: ❌ Failed\n"
          fi
        done

        echo -e "$validation_summary" >> $GITHUB_STEP_SUMMARY

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "✅ Perf pytest suite completed across all policies." >> $GITHUB_STEP_SUMMARY
