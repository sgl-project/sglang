name: Nightly Test (NPU)

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - "python/sglang/version.py"

concurrency:
  group: nightly-test-npu-${{ github.ref }}
  cancel-in-progress: true

jobs:

  ### Building images
  build-image:
    if: github.repository == 'sgl-project/sglang'
    uses: ./.github/workflows/release-docker-npu-nightly.yml
    secrets: inherit

  nightly-test-1-ascend-a3:
    needs: [ build-image ]
    if: ${{ needs.build-image.result == 'success' && (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request') }}
    runs-on: linux-aarch64-a3-2
    strategy:
      fail-fast: false
      matrix:
        part: [0, 1]
    container:
      image: ${{ vars.REGISTRY_HK }}/base_image/ascend-ci/sglang:main  # need modify
    steps:
      - name: Pre-config git access token
        run: |
          # as we use a proxy but checkout@v4 only set basic auth header for github.com
          # so we set the extraheader manually
          TOKEN=`echo -n "x-access-token:${{ secrets.ADMIN_PAT }}"|base64`
          git config --global http.https://gh-proxy.test.osinfra.cn/.extraheader "AUTHORIZATION: basic $TOKEN"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Copy dataset
        run: |
          # copy required file from our daily cache
          cp ~/.cache/modelscope/hub/datasets/otavia/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json /tmp
          # copy download through proxy
          curl -o /tmp/test.jsonl -L https://gh-proxy.test.osinfra.cn/https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl

      - name: Print Log Information
        run: |
          bash scripts/ci/log_print.sh

      - name: Run test
        timeout-minutes: 240
        env:
          SGLANG_USE_MODELSCOPE: true
          SGLANG_IS_IN_CI: true
          HF_ENDPOINT: https://hf-mirror.com
          TORCH_EXTENSIONS_DIR: /tmp/torch_extensions
          STREAMS_PER_DEVICE: 32
        run: |
          pip install sentence_transformers accelerate
          cd test/srt
          python3 run_suite.py --suite daily-test-1-ascend-npu --timeout-per-file 3600 --auto-partition-id ${{ matrix.part }} --auto-partition-size 2

  nightly-test-2-ascend-a3:
    needs: [ build-image ]
    if: ${{ needs.build-image.result == 'success' && (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request') }}
    runs-on: linux-aarch64-a3-2
    strategy:
      fail-fast: false
      matrix:
        part: [0]
    container:
      image: ${{ vars.REGISTRY_HK }}/base_image/ascend-ci/sglang:main
    steps:
      - name: Pre-config git access token
        run: |
          # as we use a proxy but checkout@v4 only set basic auth header for github.com
          # so we set the extraheader manually
          TOKEN=`echo -n "x-access-token:${{ secrets.ADMIN_PAT }}"|base64`
          git config --global http.https://gh-proxy.test.osinfra.cn/.extraheader "AUTHORIZATION: basic $TOKEN"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Copy dataset
        run: |
          # copy required file from our daily cache
          cp ~/.cache/modelscope/hub/datasets/otavia/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json /tmp
          # copy download through proxy
          curl -o /tmp/test.jsonl -L https://gh-proxy.test.osinfra.cn/https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl

      - name: Print Log Information
        run: |
          bash scripts/ci/log_print.sh

      - name: Run test
        timeout-minutes: 240
        env:
          SGLANG_USE_MODELSCOPE: true
          SGLANG_IS_IN_CI: true
          HF_ENDPOINT: https://hf-mirror.com
          TORCH_EXTENSIONS_DIR: /tmp/torch_extensions
          STREAMS_PER_DEVICE: 32
        run: |
          pip install accelerate
          cd test/srt
          python3 run_suite.py --suite daily-test-2-ascend-npu --timeout-per-file 3600 --auto-partition-id ${{ matrix.part }} --auto-partition-size 1

  nightly-test-4-ascend-a3:
    needs: [ build-image ]
    if: ${{ needs.build-image.result == 'success' && (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request') }}
    runs-on: linux-aarch64-a3-4
    strategy:
      fail-fast: false
      matrix:
        part: [0]
    container:
      image: ${{ vars.REGISTRY_HK }}/base_image/ascend-ci/sglang:main
    steps:
      - name: Pre-config git access token
        run: |
          # as we use a proxy but checkout@v4 only set basic auth header for github.com
          # so we set the extraheader manually
          TOKEN=`echo -n "x-access-token:${{ secrets.ADMIN_PAT }}"|base64`
          git config --global http.https://gh-proxy.test.osinfra.cn/.extraheader "AUTHORIZATION: basic $TOKEN"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Copy dataset
        run: |
          # copy required file from our daily cache
          cp ~/.cache/modelscope/hub/datasets/otavia/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json /tmp
          # copy download through proxy
          curl -o /tmp/test.jsonl -L https://gh-proxy.test.osinfra.cn/https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl

      - name: Print Log Information
        run: |
          bash scripts/ci/log_print.sh

      - name: Run test
        timeout-minutes: 240
        env:
          SGLANG_USE_MODELSCOPE: true
          SGLANG_IS_IN_CI: true
          HF_ENDPOINT: https://hf-mirror.com
          TORCH_EXTENSIONS_DIR: /tmp/torch_extensions
          STREAMS_PER_DEVICE: 32
        run: |
          hf download lmms-lab/MMMU --repo-type dataset
          pip install sentence_transformers torchaudio==2.8.0 torch_npu==2.8.0
          pip install protobuf==3.20 zss pre-commit wandb>=0.16.0 tenacity==8.3.0 loguru openpyxl latex2sympy2 zstandard transformers-stream-generator tqdm-multiprocess pycocoevalcap
          pip install yt-dlp sentencepiece==0.1.99 nltk av ftfy sqlitedict==2.1.0 sacrebleu>=1.5.0 pytablewriter peft==0.2.0 black==24.1.0 isort==5.13.2 peft>=0.2.0 accelerate>=0.29.1
          pip install jsonlines httpx==0.23.3 evaluate>=0.4.0 datasets==2.16.1 numexpr xgrammar==0.1.25 numpy==1.26.4
          git clone --branch v0.3.3 --depth 1 https://github.com/EvolvingLMMs-Lab/lmms-eval.git
          cd ./lmms-eval
          nohup pip install . > lmmslog.txt 2>&1 &
          sleep 120
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          cd ../
          cd test/srt
          python3 run_suite.py --suite daily-test-4-ascend-npu --timeout-per-file 3600 --auto-partition-id ${{ matrix.part }} --auto-partition-size 1

  nightly-test-16-ascend-a3:
    needs: [ build-image ]
    if: ${{ needs.build-image.result == 'success' && (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request') }}
    runs-on: linux-aarch64-a3-16
    strategy:
      fail-fast: false
      matrix:
        part: [0]
    container:
      image: ${{ vars.REGISTRY_HK }}/base_image/ascend-ci/sglang:main
    steps:
      - name: Pre-config git access token
        run: |
          # as we use a proxy but checkout@v4 only set basic auth header for github.com
          # so we set the extraheader manually
          TOKEN=`echo -n "x-access-token:${{ secrets.GITHUB_TOKEN}}"|base64`
          git config --global http.https://gh-proxy.test.osinfra.cn/.extraheader "AUTHORIZATION: basic $TOKEN"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Copy dataset
        run: |
          # copy required file from our daily cache
          cp ~/.cache/modelscope/hub/datasets/otavia/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json /tmp
          # copy download through proxy
          curl -o /tmp/test.jsonl -L https://gh-proxy.test.osinfra.cn/https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl

      - name: Print Log Information
        run: |
          bash scripts/ci/log_print.sh

      - name: Run test
        timeout-minutes: 240
        env:
          SGLANG_USE_MODELSCOPE: true
          SGLANG_IS_IN_CI: true
          HF_ENDPOINT: https://hf-mirror.com
          TORCH_EXTENSIONS_DIR: /tmp/torch_extensions
        run: |
          hf download lmms-lab/MMMU --repo-type dataset
          pip install sentence_transformers torchaudio==2.8.0 torch_npu==2.8.0
          pip install protobuf==3.20 zss pre-commit wandb>=0.16.0 tenacity==8.3.0 loguru openpyxl latex2sympy2 zstandard transformers-stream-generator tqdm-multiprocess pycocoevalcap
          pip install yt-dlp sentencepiece==0.1.99 nltk av ftfy sqlitedict==2.1.0 sacrebleu>=1.5.0 pytablewriter peft==0.2.0 black==24.1.0 isort==5.13.2 peft>=0.2.0 accelerate>=0.29.1
          pip install jsonlines httpx==0.23.3 evaluate>=0.4.0 datasets==2.16.1 numexpr xgrammar==0.1.25 numpy==1.26.4
          git clone --branch v0.3.3 --depth 1 https://github.com/EvolvingLMMs-Lab/lmms-eval.git
          cd ./lmms-eval
          nohup pip install . > lmmslog.txt 2>&1 &
          sleep 120
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          cd ../
          cd test/srt
          python3 run_suite.py --suite daily-test-16-ascend-npu --timeout-per-file 7200 --auto-partition-id ${{ matrix.part }} --auto-partition-size 1

  nightly-test-npu-finish:
    if: always()
    needs:
      - nightly-test-1-ascend-a3
      - nightly-test-2-ascend-a3
      - nightly-test-4-ascend-a3
      - nightly-test-16-ascend-a3
    runs-on: linux-amd64-cpu-1
    container:
      image: docker.m.daocloud.io/ubuntu:22.04
    steps:
       - name: Check all dependent job statuses
         run: |
           #!/bin/sh
           results="${{ join(needs.*.result, ' ') }}"
           set -- $results
           for result in "$@"; do
             if [ "$result" = "failure" ] || [ "$result" = "cancelled" ]; then
               echo "Job failed with result: $result"
               exit 1
             fi
           done
           echo "All jobs completed successfully"
           exit 0
