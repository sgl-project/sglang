from __future__ import annotations

import logging
from typing import Optional

import torch

from sglang.srt.layers.parameter import (
    BasevLLMParameter,
    permute_param_layout_,
)

from sglang.srt.layers.quantization.marlin_utils import (
    apply_gptq_marlin_linear,
    check_marlin_supports_shape,
    marlin_is_k_full,
    marlin_make_empty_g_idx,
    marlin_make_workspace,
    marlin_permute_scales,
    marlin_sort_g_idx,
    marlin_zero_points,
)

from sglang.srt.layers.quantization.utils import (
    get_scalar_types,
    unpack_cols,
)

from sglang.srt.layers.quantization.gptq import GPTQKernel

from sglang.srt.utils import is_cuda

_is_cuda = is_cuda()
if _is_cuda:
    from sgl_kernel import gptq_marlin_repack


logger = logging.getLogger(__name__)
ScalarType, scalar_types = get_scalar_types()


class MarlinLinearKernel(GPTQKernel):

    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
            device = getattr(layer, "qweight").device
            c = self.kernel_config

            check_marlin_supports_shape(
                c.partition_weight_shape[1],  # out_features
                c.partition_weight_shape[0],  # in_features
                c.full_weight_shape[0],  # in_features
                c.group_size,
            )

            row_parallel = c.partition_weight_shape[0] != c.full_weight_shape[0]
            self.is_k_full = marlin_is_k_full(c.has_g_idx, row_parallel)

            # Allocate marlin workspace.
            self.workspace = marlin_make_workspace(device)

            # Default names since marlin requires empty parameters for these,
            # TODO: remove this requirement from marlin (allow optional tensors)
            self.w_q_name = "qweight"
            self.w_s_name = "scales"
            self.w_zp_name = "qzeros"
            self.w_gidx_name = "g_idx"

            def transform_w_q(x):
                assert isinstance(x, BasevLLMParameter)
                permute_param_layout_(x, input_dim=0, output_dim=1, packed_dim=0)
                x.data = gptq_marlin_repack(
                    x.data.contiguous(),
                    perm=layer.g_idx_sort_indices,
                    size_k=c.partition_weight_shape[0],
                    size_n=c.partition_weight_shape[1],
                    num_bits=c.weight_type.size_bits,
                )
                return x

            def transform_w_s(x):
                assert isinstance(x, BasevLLMParameter)
                permute_param_layout_(x, input_dim=0, output_dim=1)
                x.data = marlin_permute_scales(
                    x.data.contiguous(),
                    size_k=c.partition_weight_shape[0],
                    size_n=c.partition_weight_shape[1],
                    group_size=c.group_size,
                )
                return x

            if c.has_g_idx:
                g_idx, g_idx_sort_indices = marlin_sort_g_idx(
                    getattr(layer, self.w_gidx_name)
                )
                self._transform_param(layer, self.w_gidx_name, lambda _: g_idx)
                layer.g_idx_sort_indices = g_idx_sort_indices
            else:
                setattr(layer, self.w_gidx_name, marlin_make_empty_g_idx(device))
                layer.g_idx_sort_indices = marlin_make_empty_g_idx(device)

            if c.zero_points:
                grouped_k = (
                    c.partition_weight_shape[0] // c.group_size if c.group_size != -1 else 1
                )
                self._transform_param(
                    layer,
                    self.w_zp_name,
                    lambda x: marlin_zero_points(
                        unpack_cols(
                            x.t(),
                            c.weight_type.size_bits,
                            grouped_k,
                            c.partition_weight_shape[1],
                        ),
                        size_k=grouped_k,
                        size_n=c.partition_weight_shape[1],
                        num_bits=c.weight_type.size_bits,
                    ),
                )
            else:
                setattr(layer, self.w_zp_name, marlin_make_empty_g_idx(device))
            self._transform_param(layer, self.w_q_name, transform_w_q)
            self._transform_param(layer, self.w_s_name, transform_w_s)


    def apply_weights(
        self,
        layer: torch.nn.Module,
        x: torch.Tensor,
        bias: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        c = self.kernel_config

        w_q, w_s, w_zp, w_gidx = self._get_weight_params(layer)

        # `process_weights_after_loading` will ensure w_zp and w_gidx are not
        #  None for marlin
        return apply_gptq_marlin_linear(
            input=x,
            weight=w_q,
            weight_scale=w_s,
            weight_zp=w_zp,  # type: ignore
            g_idx=w_gidx,  # type: ignore
            g_idx_sort_indices=layer.g_idx_sort_indices,
            workspace=self.workspace,
            wtype=c.weight_type,
            input_size_per_partition=c.partition_weight_shape[0],
            output_size_per_partition=c.partition_weight_shape[1],
            is_k_full=self.is_k_full,
            bias=bias,
        )
