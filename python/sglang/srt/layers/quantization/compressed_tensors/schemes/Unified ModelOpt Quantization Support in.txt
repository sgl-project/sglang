Unified ModelOpt Quantization Support in vLLM/SGLang
Executive summary 
We aim to enable native ModelOpt quantization in vLLM, support various modelopt quantization recipes, not just default FP4 and FP8, but also AWQ, dynamic, and others, also leverage existing llm-compressor quantization functionality in vLLM.

This project aims to expand vLLM's ModelOpt quantization support from the current limited implementation (FP8 per-tensor and NVFP4) to encompass the full suite of NVIDIA Model Optimizer quantization recipes, including INT8 SmoothQuant, INT4 AWQ, W4A8 AWQ, FP8 per-channel, and dynamic token quantization. Rather than building separate implementations for each format, we propose a unified architecture that bridges ModelOpt checkpoints to vLLM's existing compressed-tensors (llm-compressor) infrastructure, enabling code reuse, kernel sharing, and consistent performance across quantization methods. 

This approach will reduce development and maintenance burden by ~70% (leveraging 10+ existing compressed-tensors schemes instead of reimplementing each), ensure feature parity between ModelOpt and llm-compressor workflows, and provide users with seamless access to NVIDIA's enterprise-grade quantization toolkit while maintaining vLLM's high-performance inference capabilities. 

The design also has strategic implications for the broader inference ecosystem, especially SGLang. We have outlined three options to enhance ModelOpt quantization support in SGLang, pending further discussion with the relevant stakeholders.

Goals
Expand quantization format coverage: Support all major ModelOpt recipes including INT8 SmoothQuant, INT4 AWQ, W4A8 AWQ, FP8 per-channel/per-token, and dynamic quantization beyond the current FP8 per-tensor and NVFP4
Maximize code reuse: Bridge ModelOpt checkpoints to existing compressed-tensors infrastructure, leveraging 10+ existing quantization schemes and kernel implementations to reduce redundant code by ~70%
Ensure performance parity: Maintain identical inference performance between ModelOpt and llm-compressor workflows by sharing the same underlying kernels (torch.scaled_mm, CUTLASS, Marlin, FlashInfer)
Simplify maintenance: Create a unified config parser and weight loader architecture that eliminates duplicate implementations and reduces long-term maintenance burden
Enable seamless migration: Provide automatic detection and routing of ModelOpt checkpoints so users can deploy quantized models without manual configuration changes

Context
NVIDIA Model Optimizer (ModelOpt) has become the de facto standard for quantizing large language models in enterprise environments, offering sophisticated calibration algorithms (SmoothQuant, AWQ) and diverse quantization formats (FP8, INT8, INT4, W4A8) tailored for different deployment scenarios. However, vLLMâ€”the leading high-performance inference engineâ€”currently supports only two ModelOpt formats (FP8 per-tensor and NVFP4), forcing users to either re-quantize their models with llm-compressor or accept suboptimal quantization strategies. This limitation creates friction in production pipelines where models are quantized with ModelOpt's advanced tooling but cannot be fully leveraged in vLLM deployments. Meanwhile, vLLM has already invested significantly in compressed-tensors infrastructure supporting 10+ quantization schemes, creating an opportunity to bridge these ecosystems rather than maintaining parallel implementations. 
Current Modelopt quantized checkpoint deployment workflow

Take a modelopt fp8 checkpoint as an example, the following describes the workflow when run 


vllm serve nvidia/Llama-3.1-8B-Instruct-FP8 --quantization modelopt --trust-remote-code

1. Config Construction Phase
The configuration is constructed in several steps:
a) Entry Point 
The vllm serve command is handled by ServeSubcommand
Arguments are parsed and the server initialization begins
b) Model Config Creation
The ModelConfig class is initialized with:
Model name/path: nvidia/Llama-3.1-8B-Instruct-FP8
Quantization method: modelopt
Other engine parameters

c) Quantization Config Override
The _verify_quantization() method checks for quantization method overrides in order of preference. ModelOpt is in the overrides list.
d) Get Quant Config
The get_quant_config() function loads the detailed quantization config:
Looks for config.json or hf_quant_config.json file in the model directory
Loads the JSON config
For ModelOpt, checks that producer.name == "modelopt"
Calls ModelOptFp8Config.from_config() to create the config object

2. Model Initialization Phase
a) Model Architecture Selection
The initialize_model() function:
Determines the model class based on architecture (e.g., LlamaForCausalLM)
Configures the quantization config for the model class
Creates the model instance with vllm_config
b) Layer Creation with Quantization Methods
When the model is constructed, each layer calls the quantization config's get_quant_method().
This returns different method classes for different layer types:
Linear layers: ModelOptFp8LinearMethod
Attention layers: ModelOptFp8KVCacheMethod
MoE layers: ModelOptFp8MoEMethod
c) Weight Parameter Creation
For each linear layer, create_weights() is called. This creates:
weight: FP8 tensor (torch.float8_e4m3fn) with shape [output_size, input_size]
weight_scale: Per-tensor FP32 scale for weights
input_scale: Per-tensor FP32 scale for activations

3. Weight Loading Phase
a) Loading Weights
The load_weights() method:
Iterates through all safetensors/pytorch bin files
Calls model.load_weights() with an iterator of (name, tensor) pairs
b) Weight Mapping and Loading
For Llama models, the load_weights() method in the model class:
Maps HuggingFace weight names to vLLM parameter names
Uses the weight_loader function attached to each parameter
The ModelWeightParameter and PerTensorScaleParameter have custom weight loaders that handle:
Tensor parallelism sharding
Scale parameter broadcasting
c) Post-Loading Weight Processing
After all weights are loaded, process_weights_after_loading() is called.
Requantizes if there are multiple weight scales (for tensor parallel sharding)
Transposes the weight from [output, input] to [input, output] for kernel compatibility
Takes max of scales across partitions to ensure correctness
Converts to Parameter objects

4. Inference / Kernel Execution Phase
When inference runs, the apply() method is called for each layer. This delegates to Fp8LinearOp.apply() which:
Quantizes input (if needed): Converts BF16/FP16 activations to FP8
Calls FP8 GEMM kernel: Uses torch._scaled_mm() or custom CUTLASS kernels
Input: FP8 activations with input_scale
Weight: FP8 weights (transposed) with weight_scale
Output: FP16/BF16 result
Applies bias if present
Returns the output
The actual kernel selection depends on:
GPU architecture (SM 8.9+ for FP8)
Whether static or dynamic quantization is used
Tensor shapes and alignment
5. MoE-Specific Processing
For MoE layers, the workflow is similar but more complex. MoE processing includes:
w13_weight: Combined w1 and w3 weights for all experts
w2_weight: w2 weights for all experts
Separate scales for w1, w3, and w2
Post-processing requantizes experts to use unified scales
Special handling for FlashInfer or TensorRT-LLM kernels

Summary
The complete workflow is:
Config Construction: Parse HF config â†’ Detect ModelOpt â†’ Load quantization config in config.json or hf_quant_config.json â†’ Create ModelOptFp8Config
Model Init: Create model â†’ For each layer, call get_quant_method() â†’ Call create_weights() to allocate FP8 tensors and scales
Weight Loading: Load safetensors â†’ Map names â†’ Load into parameters using custom weight loaders
Post-Processing: Transpose weights, requantize if needed, unify scales across partitions
Inference: Call apply() â†’ Quantize inputs â†’ Call FP8 GEMM kernels â†’ Return output
Current RedHatAI quantized checkpoint deployment workflow
The RedHadAIâ€™s fp8 checkpoints are per-channel weights and dynamic per-token activations. 

The compressed-tensors (llm-compressor) workflow differs from ModelOpt in several key ways:
Flexible quantization strategies: Supports per-tensor, per-channel, per-block, and per-token quantization
Target-based configuration: Uses config_groups with target patterns to apply different schemes to different layers
Dynamic activation quantization: Most llm-compressor models use dynamic per-token quantization (FP8-dynamic)
No pre-computed activation scales: For dynamic models, activation scales are computed at runtime
Channel-aware processing: Per-channel scales are preserved through tensor parallelism
Scheme-based architecture: Uses CompressedTensorsScheme objects (W8A8Fp8, W8A16Fp8, etc.) for modular quantization support

Methodology and Design

llm-compressor provides a solid methodology across quantization strategy, weight and activation scaling, target matching, scale parameter management, weight processing, and kernel selection. Building on this, we propose a comprehensive design to extend ModelOptâ€™s quantization support in vLLM by leveraging the existing compressed-tensors infrastructure.
Proposal: Unified ModelOpt-Compressed-Tensors Bridge
Architecture Overview:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                    ModelOpt Checkpoint                       
â”‚  (config.json + safetensors with scales)          
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€e
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚           ModelOptUnifiedConfig (NEW)                        
â”‚  â€¢ Parse quantization config in config.json                               
â”‚  â€¢ Detect quantization scheme (FP8, FP4, INT4, W4A8, etc.)      
â”‚  â€¢ Extract strategy (per-tensor, per-channel, per-token)    
â”‚  â€¢ Map to compressed-tensors QuantizationArgs               
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      Scheme Mapper (Reuse Compressed-Tensors Schemes)       
â”‚  â€¢ FP8_DEFAULT â†’ CompressedTensorsW8A8Fp8                   
â”‚  â€¢ FP8_PER_CHANNEL â†’ CompressedTensorsW8A8Fp8 (channel)     
â”‚  â€¢ INT4_AWQ â†’ CompressedTensorsWNA16 (marlin)               
â”‚  â€¢ W4A8_AWQ â†’ CompressedTensorsW4A8Fp8                      
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚    Weight Loader Adapter (Handle ModelOpt naming)           
â”‚  â€¢ Map ModelOpt weight names to vLLM parameter names        
â”‚  â€¢ Handle scale parameter differences                        
â”‚  â€¢ Convert checkpoint format if needed                       
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚        Reuse Existing Kernels                      
â”‚  â€¢ Same kernel implementations                               
â”‚  â€¢ Same forward pass logic                                   
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Implementation Strategy
Phase 1: Create Unified ModelOpt Config Parser
Phase 2: Weight Loader Adapter
Handle naming differences between ModelOpt and compressed-tensors
Phase 3: Update Config Detection
Modify the config detection logic to route ModelOpt configs appropriately
Phase 4: Registration
Register the unified config.

Feature Matrix
After implementation, vLLM would support:
ModelOpt Recipe
Mapped to
Kernel
Status
FP8_DEFAULT_CFG
Current ModelOptFp8Config
torch._scaled_mm
âœ… Existing
FP8_PER_CHANNEL_PER_TOKEN_CFG
CompressedTensorsW8A8Fp8 (channel)
torch._scaled_mm
ğŸ†• New
INT8_DEFAULT_CFG
CompressedTensorsW8A8Int8
CUTLASS INT8
ğŸ†• New
INT8_SMOOTHQUANT_CFG
CompressedTensorsW8A8Int8
CUTLASS INT8
ğŸ†• New
INT4_AWQ_CFG
CompressedTensorsWNA16
Marlin
ğŸ†• New
W4A8_AWQ_BETA_CFG
CompressedTensorsW4A8Fp8
CUTLASS W4A8
ğŸ†• New
NVFP4
Current ModelOptNvFp4Config
CUTLASS/FlashInfer FP4
âœ… Existing



Extended design: Hybrid architecture 
While the unified bridge design works well for formats supported by both ModelOpt and llm-compressor, ModelOpt may introduce proprietary or experimental quantization formats that don't have compressed-tensors equivalents (e.g., custom NVIDIA GPU-specific formats, experimental mixed-precision schemes, or specialized formats for new hardware architectures).

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚           ModelOptUnifiedConfig (Enhanced)                   
â”‚  â€¢ Parse ModelOpt config                                    
â”‚  â€¢ Try to map to compressed-tensors scheme                  
â”‚  â€¢ If mapping fails, use ModelOpt-native scheme             
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                     â”‚                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ CT Scheme              |    |      ModelOpt-Native   
         â”‚ (Reuse)                |    |      Scheme (Custom)   
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Start with CT-only design and add native scheme support incrementally as needed:
Phase 1: Implement CT-bridge for all common formats (FP8, INT8, INT4, W4A8)
Phase 2 (if needed): Add native scheme infrastructure for ModelOpt-exclusive formats
Ongoing: Add specific native schemes only when compressed-tensors doesn't support them
Roadmap and Action Items

We need 2 full-time engineers to work on the implementation.
Engineer 1 (E1): Focus on config parsing, detection, and infrastructure
Engineer 2 (E2): Focus on weight loading, scheme mapping, and testing

Timeline: 6 - 8 weeks
Week 1: Foundation & Infrastructure - Core Infrastructure
Engineer 1:
Create ModelOptQuantizationScheme class
Implement scheme detection logic
Update config detection in model.py
Write unit tests for config parser
Engineer 2:
Create ModelOptWeightLoaderAdapter class
  
Create mock checkpoint for testing
Write adapter unit tests

Week 2: Foundation & Infrastructure - Integration Layer
Engineer 1:
Implement ModelOptUnifiedConfig class
Register unified config
Add config override detection
Integration tests for config loading
Engineer 2:
Implement ModelOptCompressedTensorsLinearMethod
Handle special cases (QKV, gate-up, fused layers)
KV cache support
Weight loading integration tests

Week 3: Recipe Implementation - FP8 Recipes
Engineer 1:
Implement FP8 per-channel config mapping
Test FP8 per-channel
Add FP8 block quantization support
Engineer 2:
Implement INT8 config mapping
Create INT8 test checkpoint
Test INT8 inference

Week 4: Recipe Implementation - INT4 & Mixed Precision
Engineer 1:
Implement INT4 AWQ config mapping
Create INT4 test checkpoint
Test INT4 AWQ inference
Engineer 2:
Implement W4A8 config mapping
Create W4A8 test checkpoint
Test W4A8 inference

Week 5: MoE & Advanced Features
Both Engineers (Collaborative):
Analyze MoE requirements
Test MoE inference
Engineer 1:
Create unified MoE method
Engineer 2:
Implement MoE weight loading

Week 6: Testing & Validation - Comprehensive Testing
Engineer 1:
Create end-to-end test suite
Performance benchmarking
Accuracy validation
Engineer 2:
Implement error handling
Test edge cases
Fix discovered bugs

Week 7: Documentation & Release - Documentation
Engineer 1:
Write architecture documentation
Update API documentation
Create migration guide
Engineer 2:
Write user guide
Create example notebooks

Week 8: Documentation & Release - Release Preparation
Engineer 1:
Update CI pipeline
Create release checklist
Write release notes
Final review & merge
Engineer 2:
Create demo checkpoints
Write blog post
Community engagement

Dependencies


Deliverables Summary
Week
E1 Deliverables
E2 Deliverables
1
Config parser + tests
Weight adapter + tests
2
Unified config class
Linear method bridge
3
FP8 per-channel support
INT8 support
4
INT4 AWQ support
W4A8 support
5
MoE method (collaborative)
MoE weight loading
6
E2E tests + benchmarks
Error handling + edge cases
7
Technical docs
User guide + examples
8
CI/CD + release
Demo checkpoints + blog

Risk assessment

Risk
Mitigation
Owner
Checkpoint format incompatibility
Early validation with real checkpoints
E2
Performance regression
Continuous benchmarking
E1
Breaking changes to compressed-tensors
Pin dependencies, coordinate with vllm team
E1
MoE complexity
Allocate extra time buffer
Both
Scale format mismatches
Comprehensive testing of all strategies
E2


Strategic Analysis: ModelOpt Quantization in SGLang
SGLang is an independent inference engine with its own architecture, currently supporting only ModelOpt FP8 and NVFP4. Previously, we have integrated native modelopt quantization support in SGLang, which enables developers to call modelopt APIs directly and deploy with SGLang runtime.

Unlike vLLM, SGLang lacks the compressed-tensors infrastructure, presenting a strategic decision point.

Option Analysis
Option 1: Port Compressed-Tensors to SGLang First
SGLang â†’ Add compressed-tensors â†’ Use vLLM's bridge design
Option 2: Native Design for SGLang
SGLang â†’ Native ModelOpt schemes â†’ Direct kernel calls
Option 3: Shared Kernel Library Approach 

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Shared Kernel Lib  
                    â”‚  (New Package)      
                    â”‚  - FP8 kernels      
                    â”‚  - INT4/INT8        
                    â”‚  - Marlin/CUTLASS   
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚    vLLM               â”‚            â”‚   SGLang               â”‚
         â”‚  - CT bridge          â”‚            â”‚  - Native              â”‚
         â”‚  - Unified            â”‚            â”‚  - Direct              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Contact: 
If any questions about this proposal and design, please reach out to Zhiyu Cheng US
