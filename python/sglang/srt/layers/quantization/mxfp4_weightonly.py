"""
MXFP4 Weight-Only GEMM for memory-efficient inference on RTX 5090 (SM120)

This module implements tile-wise dequantization with GEMM that never materializes
full BF16 weights, keeping memory usage at ~13-16GB instead of 30GB.
"""

import logging
import os
from typing import Optional, Tuple

import torch
import triton
import triton.language as tl

logger = logging.getLogger(__name__)

# Try to import native grouped kernels
try:
    from .kernels import _mxfp4_kernels as _mxfp4
    _HAVE_NATIVE_GROUPED = True
    logger.info("Native MXFP4 grouped kernels loaded successfully")
except Exception as e:
    _HAVE_NATIVE_GROUPED = False
    _mxfp4 = None
    logger.debug(f"Native MXFP4 kernels not available: {e}")


def use_weightonly_mxfp4() -> bool:
    """Check if we should use weight-only MXFP4 path."""
    if os.getenv("SGLANG_MXFP4_WEIGHTONLY", "1") != "1":
        return False
    
    if not torch.cuda.is_available():
        return False
    
    # Check for SM120+ (Blackwell and newer)
    device = torch.cuda.current_device()
    capability = torch.cuda.get_device_capability(device)
    major, minor = capability
    
    # SM120 = compute capability 12.0
    return major >= 12


@triton.jit
def mxfp4_weight_only_gemm_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr,
    # Scale pointers  
    scale_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    # Meta-parameters
    GROUP_SIZE: tl.constexpr,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    """
    Compute C = A @ B where B is MXFP4 quantized.
    
    This kernel dequantizes B tile-by-tile without materializing full BF16 weights.
    """
    # Program ID
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = num_pid_m * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * num_pid_m
    group_size_m = min(num_pid_m, M - first_pid_m * BLOCK_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # Create block pointers
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Load A tile
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        
        # Load packed MXFP4 weights (2 FP4 values per byte)
        b_packed = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)
        
        # Load scales for this group
        scale_idx = k * BLOCK_SIZE_K // GROUP_SIZE
        scale = tl.load(scale_ptr + scale_idx)
        
        # Dequantize MXFP4 to BF16 (simplified - real implementation needs proper unpacking)
        # This is where we convert packed int4 to bf16 using the scale
        b = b_packed.to(tl.bfloat16) * scale
        
        # Compute GEMM for this tile
        acc += tl.dot(a, b)
        
        # Advance pointers
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
    
    # Convert accumulator to output dtype
    c = acc.to(tl.bfloat16)
    
    # Store output
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)


def mxfp4_weight_only_gemm(
    x: torch.Tensor,
    w_packed: torch.Tensor,
    w_scale: torch.Tensor,
    group_size: int = 128,
    pack_layout: str = "mxfp4_row",
    sm: str = "120"
) -> torch.Tensor:
    """
    Perform weight-only GEMM with MXFP4 quantized weights.
    
    This function never materializes full BF16 weights, keeping memory usage low.
    
    Args:
        x: Input tensor [M, K] in BF16
        w_packed: Packed MXFP4 weights (stays compressed)
        w_scale: Per-group scales
        group_size: Quantization group size
        pack_layout: Weight packing layout
        sm: Target SM architecture
        
    Returns:
        Output tensor [M, N] in BF16
    """
    assert x.dtype == torch.bfloat16, f"Input must be BF16, got {x.dtype}"
    
    M, K = x.shape
    K_packed, N = w_packed.shape  # K is packed (K_packed = K // 2 for MXFP4)
    
    # Allocate output with zeros for in-place accumulation
    output = torch.zeros((M, N), dtype=torch.bfloat16, device=x.device)
    
    # Configure kernel
    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 128  
    BLOCK_SIZE_K = 64
    
    # Launch configuration
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    
    # For now, use a simpler fallback that at least doesn't OOM
    # Real implementation would call the Triton kernel above
    if os.getenv("SGLANG_MXFP4_USE_TRITON", "0") == "1":
        # Launch Triton kernel (needs proper unpacking logic)
        mxfp4_weight_only_gemm_kernel[grid](
            x, w_packed, output,
            w_scale,
            M, N, K,
            x.stride(0), x.stride(1),
            w_packed.stride(0), w_packed.stride(1),
            output.stride(0), output.stride(1),
            GROUP_SIZE=group_size,
            BLOCK_SIZE_M=BLOCK_SIZE_M,
            BLOCK_SIZE_N=BLOCK_SIZE_N,
            BLOCK_SIZE_K=BLOCK_SIZE_K,
        )
    else:
        # Fallback: tile over K and N to cap peak memory
        from triton_kernels.numerics_details.mxfp import upcast_from_mxfp
        
        tile_k, tile_n = 4096, 2048
        for ks in range(0, K, tile_k):
            ke = min(ks + tile_k, K)
            # Dequantize only K-slab â†’ [ke-ks, N] BF16
            # Note: w_packed is [K_packed, N] where K_packed = K // 2
            ks_packed = ks // 2
            ke_packed = (ke + 1) // 2  # Round up for odd K
            wk = upcast_from_mxfp(
                w_packed[ks_packed:ke_packed],
                w_scale[ks // group_size : (ke-1) // group_size + 1],
                dtype=torch.bfloat16, axis=-1
            ).contiguous()
            # Ensure the unpacked weight has correct shape
            if wk.shape[0] != (ke - ks):
                wk = wk[:ke-ks]  # Trim if needed
            
            xk = x[:, ks:ke].contiguous()
            for ns in range(0, N, tile_n):
                ne = min(ns + tile_n, N)
                output[:, ns:ne].add_(xk @ wk[:, ns:ne])
            del wk
            if os.getenv("SGLANG_DEBUG_EMPTY_CACHE", "0") == "1":
                torch.cuda.empty_cache()
    
    return output


def check_mxfp4_memory_usage():
    """Check if MXFP4 weight-only path is reducing memory as expected."""
    if not torch.cuda.is_available():
        return
    
    torch.cuda.synchronize()
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    
    logger.info(f"MXFP4 Weight-Only Memory Check:")
    logger.info(f"  Allocated: {allocated:.2f} GB")
    logger.info(f"  Reserved: {reserved:.2f} GB")
    
    if allocated > 20:
        logger.warning(f"Memory usage ({allocated:.2f} GB) higher than expected for weight-only path!")
        logger.warning("Check for BF16 weight materialization in the call stack")


def mxfp4_weight_only_gemm_grouped(
    X_list: list,   # [M_i, K] BF16 (contiguous)
    Wq_list: list,  # packed FP4 weights (layout-consistent)
    S_list: list,   # per-group scales (FP16/BF16)
    group_size: int = 128,
    pack_layout: str = "mxfp4_row",
    sm: str = "120",
) -> list:
    """
    Grouped weight-only GEMM wrapper with stable signature for multiple experts.
    
    This reduces kernel launch overhead and improves SM occupancy.
    Keeps a stable Python signature so we can swap between:
    (a) sequential, (b) multi-stream, (c) true grouped kernel
    without touching the MoE code.
    
    Args:
        X_list: List of input tensors [M_i, K] in BF16 (contiguous)
        Wq_list: List of packed MXFP4 weights (layout-consistent)
        S_list: List of per-group scales (FP16/BF16)
        group_size: Quantization group size
        pack_layout: Weight packing layout
        sm: Target SM architecture
        
    Returns:
        List of output tensors [M_i, N] in BF16
    """
    # Validate inputs
    assert len(X_list) == len(Wq_list) == len(S_list), \
        f"Input lists must have same length: {len(X_list)}, {len(Wq_list)}, {len(S_list)}"
    
    if not X_list:
        return []
    
    # Check dtypes and devices
    device = X_list[0].device
    for x in X_list:
        assert x.dtype == torch.bfloat16, f"Input must be BF16, got {x.dtype}"
        assert x.device == device, "All inputs must be on same device"
    
    # Use native grouped kernel if available
    if _HAVE_NATIVE_GROUPED:
        sm_arch = 120 if sm == "120" else int(sm)
        return _mxfp4.grouped_forward(X_list, Wq_list, S_list, group_size, pack_layout, sm_arch)
    
    # Fallback: Python multi-stream or sequential execution
    outputs = []
    
    # Optional: Use multi-stream for overlap
    num_streams = int(os.getenv("SGLANG_MOE_STREAMS", "1"))
    if num_streams > 1 and len(X_list) > 1:
        streams = [torch.cuda.Stream() for _ in range(min(num_streams, len(X_list)))]
        
        for i, (x, w, s) in enumerate(zip(X_list, Wq_list, S_list)):
            stream = streams[i % len(streams)]
            with torch.cuda.stream(stream):
                y = mxfp4_weight_only_gemm(x, w, s, group_size, pack_layout, sm)
                # Multi-stream overlap guard: record the producing stream
                # This prevents early frees and ensures visibility
                y.record_stream(stream)
                outputs.append(y)
        
        # Wait for all streams to ensure visibility on current stream
        for stream in streams:
            torch.cuda.current_stream().wait_stream(stream)
    else:
        # Sequential execution (single stream)
        for x, w, s in zip(X_list, Wq_list, S_list):
            y = mxfp4_weight_only_gemm(x, w, s, group_size, pack_layout, sm)
            outputs.append(y)
    
    return outputs