# é€šä¿¡æ“ä½œæ€§èƒ½æµ‹è¯•æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•æ­£ç¡®æµ‹è¯•è‡ªå®šä¹‰é€šä¿¡æ“ä½œï¼ˆPyNcclï¼‰çš„æ€§èƒ½æå‡ï¼Œä»¥åŠå“ªäº› diffusion æ¨¡å‹ä¼šç”¨åˆ°è¿™äº›æ“ä½œã€‚

## ğŸ” å“ªäº›æ¨¡å‹ä¼šä½¿ç”¨é€šä¿¡æ“ä½œï¼Ÿ

### 1. ä½¿ç”¨ Tensor Parallelism (TP) çš„æ¨¡å‹

**æ¨¡å‹**: Flux 2, QwenImage

**é€šä¿¡åœºæ™¯**:
- `all_reduce`: åœ¨ linear å±‚çš„è¾“å‡ºèšåˆ
- `all_gather`: åœ¨ embedding å±‚æ”¶é›†ç»“æœ

**æµ‹è¯•å‘½ä»¤**:
```bash
# Flux 2 with TP=2
sglang generate \
  --model-path black-forest-labs/FLUX.1-dev \
  --prompt "A beautiful landscape" \
  --tp-size 2 \
  --perf-dump-path flux_tp2.json

# Flux 2 with TP=4
sglang generate \
  --model-path black-forest-labs/FLUX.1-dev \
  --prompt "A beautiful landscape" \
  --tp-size 4 \
  --perf-dump-path flux_tp4.json
```

### 2. ä½¿ç”¨ Sequence Parallelism (SP) çš„æ¨¡å‹

**æ¨¡å‹**: WanVideo, HunyuanVideo, FastWan

**é€šä¿¡åœºæ™¯**:
- `all_to_all_4D`: åœ¨ attention å±‚é‡æ–°åˆ†å¸ƒåºåˆ—å’Œå¤´ç»´åº¦
- `all_gather`: æ”¶é›† replicated token çš„ç»“æœ

**ä»£ç ä½ç½®**: `runtime/layers/attention/layer.py` çš„ `UlyssesAttention`

**æµ‹è¯•å‘½ä»¤**:
```bash
# Wan 2.1 with SP (Ulysses)
sglang generate \
  --model-path Wan-AI/Wan2.1-T2V-1.3B-Diffusers \
  --prompt "A curious raccoon" \
  --ulysses-degree 2 \
  --perf-dump-path wan_sp2.json

# Wan 2.1 with SP (Ulysses) + Ring
sglang generate \
  --model-path Wan-AI/Wan2.1-T2V-1.3B-Diffusers \
  --prompt "A curious raccoon" \
  --ulysses-degree 2 \
  --ring-degree 2 \
  --perf-dump-path wan_usp22.json
```

### 3. ä½¿ç”¨ CFG Parallel çš„æ¨¡å‹

**æ¨¡å‹**: æ‰€æœ‰æ”¯æŒ Classifier-Free Guidance çš„æ¨¡å‹

**é€šä¿¡åœºæ™¯**:
- `broadcast`: å¹¿æ’­æ¡ä»¶å’Œéæ¡ä»¶çš„ prompt embeddings
- `all_gather`: æ”¶é›† CFG å¹¶è¡Œçš„ç»“æœ

**æµ‹è¯•å‘½ä»¤**:
```bash
# ä»»ä½•æ¨¡å‹ with CFG parallel
sglang generate \
  --model-path Wan-AI/Wan2.1-T2V-1.3B-Diffusers \
  --prompt "A beautiful scene" \
  --enable-cfg-parallel \
  --num-gpus 2 \
  --perf-dump-path wan_cfg.json
```

## ğŸš€ æ­£ç¡®çš„æ€§èƒ½æµ‹è¯•æ–¹æ³•

### æ–¹æ³• 1: ç«¯åˆ°ç«¯æ¨ç†æµ‹è¯•ï¼ˆæ¨èï¼‰

å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨ device_communicator çš„æ€§èƒ½ï¼š

```bash
# 1. ä½¿ç”¨ PyNccl (device_communicator)
sglang generate \
  --model-path Wan-AI/Wan2.1-T2V-1.3B-Diffusers \
  --prompt "test prompt" \
  --ulysses-degree 2 \
  --perf-dump-path baseline_pynccl.json

# 2. ä½¿ç”¨ PyTorch distributed (éœ€è¦ä¿®æ”¹ä»£ç ç¦ç”¨ device_communicator)
# åœ¨ group_coordinator.py ä¸­æš‚æ—¶è®¾ç½® use_device_communicator=False
sglang generate \
  --model-path Wan-AI/Wan2.1-T2V-1.3B-Diffusers \
  --prompt "test prompt" \
  --ulysses-degree 2 \
  --perf-dump-path baseline_pytorch.json

# 3. å¯¹æ¯”æ€§èƒ½
python python/sglang/multimodal_gen/benchmarks/compare_perf.py \
  baseline_pytorch.json baseline_pynccl.json
```

### æ–¹æ³• 2: Attention å±‚å¾®åŸºå‡†æµ‹è¯•

åˆ›å»ºä¸“é—¨çš„ attention å±‚æµ‹è¯•ï¼š

```python
# test_attention_communication.py
import torch
import torch.distributed as dist
from sglang.multimodal_gen.runtime.distributed.parallel_state import (
    initialize_model_parallel
)
from sglang.multimodal_gen.runtime.distributed.communication_op import (
    sequence_model_parallel_all_to_all_4D
)

def benchmark_attention_comm():
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend="nccl")
    initialize_model_parallel(sequence_parallel_degree=2)
    
    # æ¨¡æ‹Ÿ attention å±‚çš„é€šä¿¡æ¨¡å¼
    rank = dist.get_rank()
    device = torch.device(f"cuda:{rank}")
    
    # å¤§å¼ é‡ï¼šæ¨¡æ‹ŸçœŸå®çš„ attention è¾“å…¥ (ç±»ä¼¼ Wan æ¨¡å‹)
    # [3(qkv), seq_len, num_heads, head_dim]
    batch_size = 1
    seq_len = 4096  # é•¿åºåˆ—
    num_heads = 16
    head_dim = 64
    
    qkv = torch.randn(3, seq_len, num_heads, head_dim, device=device)
    
    # é¢„çƒ­
    for _ in range(10):
        _ = sequence_model_parallel_all_to_all_4D(qkv, scatter_dim=2, gather_dim=1)
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    import time
    start = time.perf_counter()
    for _ in range(100):
        result = sequence_model_parallel_all_to_all_4D(qkv, scatter_dim=2, gather_dim=1)
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    if rank == 0:
        print(f"Average time per all-to-all: {elapsed/100*1000:.3f} ms")
        print(f"Tensor size: {qkv.nbytes / 1024 / 1024:.2f} MB")

if __name__ == "__main__":
    benchmark_attention_comm()
```

è¿è¡Œï¼š
```bash
torchrun --nproc_per_node=2 test_attention_communication.py
```

## ğŸ“Š é¢„æœŸçš„æ€§èƒ½æå‡

### å°å¼ é‡ï¼ˆ< 1MBï¼‰
- **é¢„æœŸæå‡**: 0-10%
- **åŸå› **: å¼€é”€å ä¸»å¯¼ï¼ŒPyNccl ä¼˜åŠ¿ä¸æ˜æ˜¾

### ä¸­ç­‰å¼ é‡ï¼ˆ1-10MBï¼‰
- **é¢„æœŸæå‡**: 10-30%
- **åŸå› **: é€šä¿¡æ—¶é—´å¼€å§‹å ä¸»å¯¼

### å¤§å¼ é‡ï¼ˆ> 10MBï¼‰
- **é¢„æœŸæå‡**: 30-50%
- **åŸå› **: PyNccl çš„ä¼˜åŒ–å®Œå…¨å‘æŒ¥ä½œç”¨

### å®é™…æ¨¡å‹æ¨ç†
- **Wan 2.1 with SP (ulysses_degree=2)**: é¢„æœŸæ•´ä½“æå‡ 5-15%
- **Flux 2 with TP (tp_size=2)**: é¢„æœŸæ•´ä½“æå‡ 3-10%
- **å¤æ‚å¹¶è¡Œé…ç½®**: é¢„æœŸæ•´ä½“æå‡ 10-25%

## ğŸ¯ å…³é”®æµ‹è¯•åœºæ™¯

### åœºæ™¯ 1: Wan è§†é¢‘ç”Ÿæˆ + Sequence Parallel

è¿™æ˜¯æœ€èƒ½ä½“ç°é€šä¿¡ä¼˜åŒ–çš„åœºæ™¯ï¼š

```bash
# æµ‹è¯•è„šæœ¬
#!/bin/bash

MODEL="Wan-AI/Wan2.1-T2V-1.3B-Diffusers"
PROMPT="A curious raccoon peers through a vibrant field of yellow sunflowers"

# ä¸åŒçš„å¹¶è¡Œé…ç½®
for SP in 1 2 4; do
  echo "Testing SP=$SP..."
  sglang generate \
    --model-path $MODEL \
    --prompt "$PROMPT" \
    --ulysses-degree $SP \
    --num-inference-steps 50 \
    --perf-dump-path "wan_sp${SP}.json"
done

# å¯¹æ¯”æ€§èƒ½
python python/sglang/multimodal_gen/benchmarks/compare_perf.py \
  wan_sp1.json wan_sp2.json > sp_comparison.txt
```

### åœºæ™¯ 2: Flux å›¾åƒç”Ÿæˆ + Tensor Parallel

```bash
#!/bin/bash

MODEL="black-forest-labs/FLUX.1-dev"
PROMPT="A professional photo of a cat wearing a tiny hat"

# ä¸åŒçš„ TP é…ç½®
for TP in 1 2 4; do
  echo "Testing TP=$TP..."
  sglang generate \
    --model-path $MODEL \
    --prompt "$PROMPT" \
    --tp-size $TP \
    --num-inference-steps 28 \
    --perf-dump-path "flux_tp${TP}.json"
done
```

## ğŸ”¬ é€šä¿¡æ“ä½œçš„è°ƒç”¨ä½ç½®

### 1. `all_reduce`
- **ä½ç½®**: `runtime/layers/linear.py` - `RowParallelLinear`
- **é¢‘ç‡**: æ¯ä¸ª attention å—å
- **å¤§å°**: å–å†³äº hidden_dim å’Œ sequence length

### 2. `all_gather`
- **ä½ç½®**: `runtime/layers/vocab_parallel_embedding.py`
- **é¢‘ç‡**: Embedding å±‚è¾“å‡º
- **å¤§å°**: å–å†³äº vocab_size å’Œ batch_size

### 3. `all_to_all_4D`
- **ä½ç½®**: `runtime/layers/attention/layer.py` - `UlyssesAttention`
- **é¢‘ç‡**: æ¯ä¸ª attention å±‚å‰åå„ä¸€æ¬¡
- **å¤§å°**: [batch, seq_len, num_heads, head_dim]

### 4. `broadcast`
- **ä½ç½®**: `runtime/pipelines_core/executors/parallel_executor.py`
- **é¢‘ç‡**: æ¯æ¬¡ forward å¼€å§‹æ—¶
- **å¤§å°**: æ•´ä¸ª batch çš„å…ƒæ•°æ®

## ğŸ’¡ è°ƒè¯•å»ºè®®

### 1. éªŒè¯ PyNccl æ˜¯å¦å¯ç”¨

```python
# åœ¨ group_coordinator.py ä¸­æ·»åŠ æ—¥å¿—
def all_reduce(self, input_, ...):
    if self.device_communicator is not None:
        logger.info(f"âœ… Using device_communicator (PyNccl)")
        result = self.device_communicator.all_reduce(input_, op=op)
    else:
        logger.info(f"âš ï¸  Fallback to PyTorch distributed")
        # ...
```

### 2. æ£€æŸ¥é€šä¿¡è€—æ—¶

```python
import time

def all_reduce(self, input_, ...):
    start = time.perf_counter()
    result = self.device_communicator.all_reduce(input_, op=op)
    elapsed = time.perf_counter() - start
    logger.info(f"all_reduce time: {elapsed*1000:.3f} ms, size: {input_.nbytes/1024/1024:.2f} MB")
    return result
```

### 3. å¯¹æ¯”ä¸åŒå®ç°

ä¸´æ—¶ä¿®æ”¹ä»£ç ï¼Œå¯¹æ¯”ä¸¤ç§å®ç°ï¼š

```python
def all_reduce(self, input_, ...):
    # æ–¹æ³• 1: PyNccl
    input_pynccl = input_.clone()
    start1 = time.perf_counter()
    result1 = self.device_communicator.all_reduce(input_pynccl, op=op)
    time1 = time.perf_counter() - start1
    
    # æ–¹æ³• 2: PyTorch
    input_torch = input_.clone()
    start2 = time.perf_counter()
    torch.distributed.all_reduce(input_torch, group=self.device_group, op=op)
    time2 = time.perf_counter() - start2
    
    logger.info(f"PyNccl: {time1*1000:.3f} ms, PyTorch: {time2*1000:.3f} ms, Speedup: {time2/time1:.2f}x")
    return result1
```

## ğŸ“ˆ æ€§èƒ½åˆ†æå·¥å…·

### ä½¿ç”¨ NCCL ç¯å¢ƒå˜é‡

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# æ€§èƒ½è°ƒä¼˜
export NCCL_IB_DISABLE=0  # å¯ç”¨ InfiniBand (å¦‚æœæœ‰)
export NCCL_P2P_DISABLE=0  # å¯ç”¨ P2P
export NCCL_SHM_DISABLE=0  # å¯ç”¨å…±äº«å†…å­˜

sglang generate --model-path ... --ulysses-degree 2 ...
```

### ä½¿ç”¨ PyTorch Profiler

```python
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    result = generator.generate(sampling_params_kwargs=dict(prompt="test"))

prof.export_chrome_trace("trace.json")
# åœ¨ chrome://tracing ä¸­æŸ¥çœ‹
```

## âœ… æ£€æŸ¥æ¸…å•

- [ ] ç¡®è®¤ä½¿ç”¨äº† `device_communicator` (æŸ¥çœ‹æ—¥å¿—)
- [ ] ç¡®è®¤ PyNccl åˆå§‹åŒ–æˆåŠŸ
- [ ] ä½¿ç”¨å¤š GPU åœºæ™¯ (TP > 1 æˆ– SP > 1)
- [ ] ä½¿ç”¨è¶³å¤Ÿå¤§çš„æ¨¡å‹å’Œåºåˆ—é•¿åº¦
- [ ] åœ¨ç«¯åˆ°ç«¯æ¨ç†ä¸­æµ‹è¯•ï¼Œè€Œä¸æ˜¯å•ç‹¬æµ‹è¯•é€šä¿¡æ“ä½œ
- [ ] å¯¹æ¯”ä¸åŒå¹¶è¡Œé…ç½®çš„æ€§èƒ½
- [ ] æ£€æŸ¥ NCCL ç‰ˆæœ¬å’Œé…ç½®

## ğŸ“ æ€»ç»“

1. **å•ç‹¬æµ‹è¯•é€šä¿¡æ“ä½œæ€§èƒ½æå‡ä¸æ˜æ˜¾æ˜¯æ­£å¸¸çš„**
2. **çœŸæ­£çš„æ€§èƒ½æå‡ä½“ç°åœ¨å®é™…æ¨¡å‹æ¨ç†ä¸­**
3. **ä½¿ç”¨ SP æˆ– TP çš„æ¨¡å‹æ‰ä¼šç”¨åˆ°è¿™äº›é€šä¿¡æ“ä½œ**
4. **æ¨èæµ‹è¯•åœºæ™¯**: Wan + Ulysses SP æˆ– Flux + TP

