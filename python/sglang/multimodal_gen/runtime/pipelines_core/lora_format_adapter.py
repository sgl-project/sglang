# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

from enum import Enum
from typing import Dict, Optional

import torch

try:
    from diffusers.loaders import lora_conversion_utils as lcu  # type: ignore[attr-defined]
except Exception:
    lcu = None  # type: ignore[assignment]


class LoRAFormat(str, Enum):
    """Supported external LoRA formats prior to normalization."""
    STANDARD = "standard"
    NON_DIFFUSERS_SD = "non-diffusers"
    XLABS_FLUX = "xlabs-ai"
    KOHYA_FLUX = "kohya-flux"
    UNKNOWN = "unknown"


# ---------------------------------------------------------------------------
# Heuristics
# ---------------------------------------------------------------------------

def _is_xlabs_ai_key(k: str) -> bool:
    """
    Identifies XLabs FLUX-style LoRA keys. These typically appear under
    double_blocks/single_blocks with LORA subkeys such as proj_lora/qkv_lora.
    """
    if not (k.endswith(".down.weight") or k.endswith(".up.weight")):
        return False

    if not k.startswith((
        "double_blocks.",
        "single_blocks.",
        "diffusion_model.double_blocks",
        "diffusion_model.single_blocks",
    )):
        return False

    return (".processor." in k) or (".proj_lora" in k) or (".qkv_lora" in k)


def _looks_like_kohya_flux(state_dict: Dict[str, torch.Tensor]) -> bool:
    """
    Detects Kohya FLUX LoRA generated by sd-scripts (flux_lora.py).
    Common prefixes: lora_unet_double_blocks_*, lora_unet_single_blocks_*.
    """
    if not state_dict:
        return False
    keys = state_dict.keys()
    return any(
        k.startswith("lora_unet_double_blocks_") or k.startswith("lora_unet_single_blocks_")
        for k in keys
    )


def _looks_like_non_diffusers_sd(state_dict: Dict[str, torch.Tensor]) -> bool:
    """
    Detects classic non-diffusers SD LoRA formats (Kohya/A1111/sd-scripts).
    Typical prefixes: lora_unet_*, lora_te_*, lora_te1_*, lora_te2_*.
    """
    if not state_dict:
        return False
    keys = state_dict.keys()
    return all(
        k.startswith(("lora_unet_", "lora_te_", "lora_te1_", "lora_te2_"))
        for k in keys
    )


# ---------------------------------------------------------------------------
# Format Detection
# ---------------------------------------------------------------------------

def detect_lora_format_from_state_dict(
    lora_state_dict: Dict[str, torch.Tensor],
) -> LoRAFormat:
    """
    Determines which LoRA format the raw state_dict uses based on key patterns.
    """
    if not lora_state_dict:
        return LoRAFormat.STANDARD

    keys = list(lora_state_dict.keys())

    if any(_is_xlabs_ai_key(k) for k in keys):
        return LoRAFormat.XLABS_FLUX

    if _looks_like_kohya_flux(lora_state_dict):
        return LoRAFormat.KOHYA_FLUX

    if _looks_like_non_diffusers_sd(lora_state_dict):
        return LoRAFormat.NON_DIFFUSERS_SD

    if any(
        (".lora_down.weight" in k) or (".lora_up.weight" in k)
        for k in keys
    ):
        return LoRAFormat.NON_DIFFUSERS_SD

    return LoRAFormat.STANDARD


# ---------------------------------------------------------------------------
# Converters
# ---------------------------------------------------------------------------

def _convert_xlabs_ai_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts XLabs FLUX LoRA into a diffusers-compatible format.
    """
    if lcu is None:
        if logger:
            logger.warning("XLabs FLUX detected but diffusers is unavailable.")
        return lora_state_dict

    candidate_names = (
        "_convert_xlabs_flux_lora_to_diffusers",
        "convert_xlabs_lora_state_dict_to_diffusers",
        "convert_xlabs_lora_to_diffusers",
        "convert_xlabs_flux_lora_to_diffusers",
    )
    converters = [(n, getattr(lcu, n)) for n in candidate_names if callable(getattr(lcu, n, None))]

    if not converters:
        if logger:
            logger.warning("No XLabs converter found in diffusers.")
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err: Optional[Exception] = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned {type(out)}")
            if logger:
                logger.info(f"Converted XLabs FLUX LoRA using {name}")
            return out
        except Exception as e:
            last_err = e

    if logger:
        logger.warning(f"All XLabs converters failed; last error: {last_err}")
    return lora_state_dict


def _convert_kohya_flux_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts Kohya FLUX LoRA into a diffusers-compatible format.
    """
    if lcu is None:
        if logger:
            logger.warning("Kohya FLUX detected but diffusers is unavailable.")
        return lora_state_dict

    candidate_names = (
        "_convert_kohya_flux_lora_to_diffusers",
        "convert_kohya_flux_lora_to_diffusers",
    )
    converters = [(n, getattr(lcu, n)) for n in candidate_names if callable(getattr(lcu, n, None))]

    if not converters:
        if logger:
            logger.warning("No Kohya FLUX converter found.")
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err: Optional[Exception] = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned {type(out)}")
            if logger:
                logger.info(f"Converted Kohya FLUX LoRA using {name}")
            return out
        except Exception as e:
            last_err = e

    if logger:
        logger.warning(f"Kohya FLUX conversion failed; last error: {last_err}")
    return lora_state_dict


def _convert_non_diffusers_sd_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts non-diffusers SD LoRA formats (SD1/2/XL, video LoRAs, transformer LoRAs)
    into diffusers-compatible naming.
    """
    if lcu is None:
        if logger:
            logger.warning("Non-diffusers LoRA detected but diffusers is unavailable.")
        return lora_state_dict

    candidate_names = (
        "_convert_non_diffusers_lora_to_diffusers",
        "convert_non_diffusers_lora_to_diffusers",
        "_convert_non_diffusers_wan_lora_to_diffusers",
        "convert_non_diffusers_wan_lora_to_diffusers",
        "_convert_musubi_wan_lora_to_diffusers",
        "convert_musubi_wan_lora_to_diffusers",
        "_convert_non_diffusers_flux2_lora_to_diffusers",
        "convert_non_diffusers_flux2_lora_to_diffusers",
        "_convert_non_diffusers_lumina2_lora_to_diffusers",
        "convert_non_diffusers_lumina2_lora_to_diffusers",
        "_convert_non_diffusers_ltxv_lora_to_diffusers",
        "convert_non_diffusers_ltxv_lora_to_diffusers",
        "_convert_non_diffusers_hidream_lora_to_diffusers",
        "convert_non_diffusers_hidream_lora_to_diffusers",
        "_convert_non_diffusers_z_image_lora_to_diffusers",
        "convert_non_diffusers_z_image_lora_to_diffusers",
        "_convert_non_diffusers_qwen_lora_to_diffusers",
        "convert_non_diffusers_qwen_lora_to_diffusers",
        "_convert_fal_kontext_lora_to_diffusers",
        "convert_fal_kontext_lora_to_diffusers",
    )

    converters = [
        (n, getattr(lcu, n)) for n in candidate_names
        if callable(getattr(lcu, n, None))
    ]

    if not converters:
        if logger:
            logger.warning("No non-diffusers converters found.")
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err: Optional[Exception] = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned {type(out)}")
            if logger:
                logger.info(f"Converted non-diffusers LoRA using {name}")
            return out
        except Exception as e:
            last_err = e

    if logger:
        logger.warning(
            f"All non-diffusers converters failed; last error: {last_err}"
        )
    return lora_state_dict


# ---------------------------------------------------------------------------
# Conversion Dispatcher
# ---------------------------------------------------------------------------

def convert_lora_state_dict_by_format(
    lora_state_dict: Dict[str, torch.Tensor],
    lora_format: LoRAFormat,
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Dispatches the state_dict through the selected converter.
    """
    if lora_format == LoRAFormat.STANDARD:
        return lora_state_dict
    if lora_format == LoRAFormat.XLABS_FLUX:
        return _convert_xlabs_ai_via_diffusers(lora_state_dict, logger)
    if lora_format == LoRAFormat.KOHYA_FLUX:
        return _convert_kohya_flux_via_diffusers(lora_state_dict, logger)
    if lora_format == LoRAFormat.NON_DIFFUSERS_SD:
        return _convert_non_diffusers_sd_via_diffusers(lora_state_dict, logger)

    if logger:
        logger.warning(f"Unknown LoRA format '{lora_format}'. Returning raw state_dict.")
    return lora_state_dict


def normalize_lora_state_dict(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Normalizes any external LoRA format into diffusers-compatible naming.
    """
    if not lora_state_dict:
        return lora_state_dict

    fmt = detect_lora_format_from_state_dict(lora_state_dict)
    return convert_lora_state_dict_by_format(lora_state_dict, fmt, logger)
