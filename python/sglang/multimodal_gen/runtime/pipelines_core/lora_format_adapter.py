# sglang/multimodal_gen/runtime/pipelines_core/lora_format_adapter.py

from __future__ import annotations

from enum import Enum
from typing import Dict, Optional

import torch

try:
    # Imported lazily because diffusers may not be installed
    from diffusers.loaders import lora_conversion_utils as lcu  # type: ignore[attr-defined]
except Exception:
    lcu = None  # type: ignore[assignment]


class LoRAFormat(str, Enum):
    """Enumerates supported external LoRA formats before normalization."""
    STANDARD = "standard"                # Already diffusers/PEFT-style
    NON_DIFFUSERS_SD = "non-diffusers"   # SD1/2/XL LoRA: Kohya / A1111 / Comfy / LyCORIS / DoRA & SD1/2/XL 以及 Wan / HunyuanVideo / LTX-Video
    XLABS_FLUX = "xlabs-ai"              # XLabs FLUX-style LoRA format
    KOHYA_FLUX = "kohya-flux"            # Kohya-style FLUX format (sd-scripts/flux_lora.py)
    UNKNOWN = "unknown"


# ----------- Format Heuristics -----------

def _is_xlabs_ai_key(k: str) -> bool:
    """
    Detects XLabs FLUX-format LoRA keys based on naming patterns.
    These LoRAs use 'double_blocks'/'single_blocks' under 'diffusion_model'
    and LoRA subkeys like proj_lora/qkv_lora.
    """
    if not (k.endswith(".down.weight") or k.endswith(".up.weight")):
        return False

    if not k.startswith((
        "double_blocks.",
        "single_blocks.",
        "diffusion_model.double_blocks",
        "diffusion_model.single_blocks",
    )):
        return False

    return (".processor." in k) or (".proj_lora" in k) or (".qkv_lora" in k)


def _looks_like_kohya_flux(state_dict: Dict[str, torch.Tensor]) -> bool:
    """
    Detects Kohya-style FLUX LoRA generated by sd-scripts/flux_lora.py.
    Patterns typically include 'lora_unet_double_blocks_*' or 'lora_unet_single_blocks_*'.
    """
    if not state_dict:
        return False

    keys = state_dict.keys()
    return any(
        k.startswith("lora_unet_double_blocks_") or k.startswith("lora_unet_single_blocks_")
        for k in keys
    )


def _looks_like_non_diffusers_sd(state_dict: Dict[str, torch.Tensor]) -> bool:
    """
    Detects classic non-diffusers SD LoRA formats (Kohya/A1111/sd-scripts).
    These LoRAs use prefixes such as:
      - lora_unet_*
      - lora_te_*
      - lora_te1_*
      - lora_te2_*
    """
    if not state_dict:
        return False
    keys = state_dict.keys()
    return all(
        k.startswith(("lora_unet_", "lora_te_", "lora_te1_", "lora_te2_"))
        for k in keys
    )


# ----------- Format Detection -----------

def detect_lora_format_from_state_dict(
    lora_state_dict: Dict[str, torch.Tensor],
) -> LoRAFormat:
    """
    Detects which external LoRA format a raw state_dict belongs to,
    based solely on parameter naming.
    """
    if not lora_state_dict:
        return LoRAFormat.STANDARD

    keys = list(lora_state_dict.keys())

    # XLabs Flux LoRA
    if any(_is_xlabs_ai_key(k) for k in keys):
        return LoRAFormat.XLABS_FLUX

    # Kohya sd-scripts Flux LoRA
    if _looks_like_kohya_flux(lora_state_dict):
        return LoRAFormat.KOHYA_FLUX

    # SD1/2/XL non-diffusers LoRA (Kohya/A1111)
    if _looks_like_non_diffusers_sd(lora_state_dict):
        return LoRAFormat.NON_DIFFUSERS_SD

    # Wan / Musubi-Wan / HunyuanVideo / LTX-Video 等：
    # 仍然是非 diffusers 命名，通常带 .lora_down/.lora_up
    if any(
        (".lora_down.weight" in k) or (".lora_up.weight" in k)
        for k in keys
    ):
        return LoRAFormat.NON_DIFFUSERS_SD

    # Otherwise assume diffusers/PEFT-style
    return LoRAFormat.STANDARD


# ----------- Converters -----------

def _convert_xlabs_ai_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts an XLabs FLUX LoRA to a diffusers-compatible format using
    diffusers' built-in converter functions.
    """
    if lcu is None:
        if logger:
            logger.warning("XLabs LoRA detected but diffusers is unavailable.")
        return lora_state_dict

    candidate_names = (
        "_convert_xlabs_flux_lora_to_diffusers",
        "convert_xlabs_lora_state_dict_to_diffusers",
        "convert_xlabs_lora_to_diffusers",
        "convert_xlabs_flux_lora_to_diffusers",
    )

    converters = [(name, getattr(lcu, name)) for name in candidate_names if callable(getattr(lcu, name, None))]

    if not converters:
        if logger:
            logger.warning("No XLabs converter found in diffusers; returning raw dict.")
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err: Optional[Exception] = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned unexpected type {type(out)}")
            if logger:
                logger.info(f"Converted XLabs FLUX LoRA using {name}")
            return out
        except Exception as e:
            last_err = e
            continue

    if logger:
        logger.warning(f"All XLabs converters failed; last error: {last_err}")
    return lora_state_dict


def _convert_kohya_flux_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts Kohya-style FLUX LoRA to diffusers format.
    """
    if lcu is None:
        if logger:
            logger.warning("Kohya Flux LoRA detected but diffusers unavailable.")
        return lora_state_dict

    candidate_names = (
        "_convert_kohya_flux_lora_to_diffusers",
        "convert_kohya_flux_lora_to_diffusers",
    )

    converters = [(name, getattr(lcu, name)) for name in candidate_names if callable(getattr(lcu, name, None))]

    if not converters:
        if logger:
            logger.warning("No Kohya Flux converter found in diffusers; returning raw dict.")
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned unexpected type {type(out)}")
            if logger:
                logger.info(f"Converted Kohya Flux LoRA using {name}")
            return out
        except Exception as e:
            last_err = e
            continue

    if logger:
        logger.warning(f"All Kohya Flux converters failed; last error: {last_err}")
    return lora_state_dict


def _convert_non_diffusers_sd_via_diffusers(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Converts classic SD1/2/XL LoRA formats (A1111/Kohya/sd-scripts)
    as well as non-diffusers video LoRAs (Wan / Musubi-Wan / HunyuanVideo / LTX-Video)
    into diffusers-compatible LoRA naming.

    Delegates to diffusers.loaders.lora_conversion_utils, trying all known
    non-diffusers converters in order and stopping at the first one that succeeds.
    """
    if lcu is None:
        if logger:
            logger.warning("Non-diffusers SD/Video LoRA detected but diffusers unavailable.")
        return lora_state_dict

    candidate_names = (
        # 传统 SD non-diffusers (Kohya/A1111/sd-scripts)
        "_convert_non_diffusers_lora_to_diffusers",
        "convert_non_diffusers_lora_to_diffusers",
        # Wan / Musubi-Wan
        "_convert_non_diffusers_wan_lora_to_diffusers",
        "convert_non_diffusers_wan_lora_to_diffusers",
        "_convert_musubi_wan_lora_to_diffusers",
        "convert_musubi_wan_lora_to_diffusers",
        # HunyuanVideo
        "_convert_hunyuan_video_lora_to_diffusers",
        "convert_hunyuan_video_lora_to_diffusers",
        # LTX-Video
        "_convert_non_diffusers_ltxv_lora_to_diffusers",
        "convert_non_diffusers_ltxv_lora_to_diffusers",
    )

    converters = [
        (name, getattr(lcu, name))
        for name in candidate_names
        if callable(getattr(lcu, name, None))
    ]

    if not converters:
        if logger:
            logger.warning(
                "No non-diffusers SD/video LoRA converters found in diffusers; returning raw dict."
            )
        return lora_state_dict

    sd_copy = dict(lora_state_dict)
    last_err: Optional[Exception] = None

    for name, fn in converters:
        try:
            out = fn(sd_copy)
            # 一些 converter 可能返回 (state_dict, metadata)
            if isinstance(out, tuple) and isinstance(out[0], dict):
                out = out[0]
            if not isinstance(out, dict):
                raise TypeError(f"Converter {name} returned unexpected type {type(out)}")
            if logger:
                logger.info(f"Converted non-diffusers SD/video LoRA using {name}")
            return out
        except Exception as e:
            last_err = e
            continue

    if logger:
        logger.warning(
            f"All non-diffusers SD/video LoRA converters failed; last error: {last_err}"
        )
    return lora_state_dict

# ----------- Normalization Dispatcher -----------

def convert_lora_state_dict_by_format(
    lora_state_dict: Dict[str, torch.Tensor],
    lora_format: LoRAFormat,
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Dispatches the state_dict to the appropriate converter based on detected format.
    """
    if lora_format == LoRAFormat.STANDARD:
        return lora_state_dict
    if lora_format == LoRAFormat.XLABS_FLUX:
        return _convert_xlabs_ai_via_diffusers(lora_state_dict, logger)
    if lora_format == LoRAFormat.KOHYA_FLUX:
        return _convert_kohya_flux_via_diffusers(lora_state_dict, logger)
    if lora_format == LoRAFormat.NON_DIFFUSERS_SD:
        return _convert_non_diffusers_sd_via_diffusers(lora_state_dict, logger)

    if logger:
        logger.warning(f"Unknown LoRA format '{lora_format}', returning raw dict.")
    return lora_state_dict


def normalize_lora_state_dict(
    lora_state_dict: Dict[str, torch.Tensor],
    logger=None,
) -> Dict[str, torch.Tensor]:
    """
    Normalizes any supported external LoRA format into diffusers-style naming.

    Steps:
      1. Detect the raw LoRA format via key heuristics.
      2. Apply the corresponding diffusers conversion function.
    """
    if not lora_state_dict:
        return lora_state_dict

    fmt = detect_lora_format_from_state_dict(lora_state_dict)
    return convert_lora_state_dict_by_format(lora_state_dict, fmt, logger)
