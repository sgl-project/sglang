# Two-stage Dockerfile for testing Anthropic API with MiniMax-M2
#
# Stage 1 (base): Heavy dependencies + cubins (~10GB, rebuild rarely)
# Stage 2 (dev): Just the code (~few MB, rebuild on each code change)
#
# Build base image (do once, push to registry):
#   docker build -f docker/Dockerfile.anthropic-test --target base \
#     -t your-registry/sglang-anthropic-base:latest \
#     --build-arg BUILD_PARALLEL=4 .
#   docker push your-registry/sglang-anthropic-base:latest
#
# Build dev image (fast, only code layer changes):
#   docker build -f docker/Dockerfile.anthropic-test \
#     -t sglang-anthropic-test \
#     --build-arg BASE_IMAGE=your-registry/sglang-anthropic-base:latest .
#
# Run server:
#   docker run --gpus all -p 8000:8000 -v $HF_HOME:/root/.cache/huggingface \
#     sglang-anthropic-test \
#     python -m sglang.launch_server \
#       --model-path MiniMax/MiniMax-M2.1 \
#       --port 8000 \
#       --tool-call-parser minimax-m2 \
#       --host 0.0.0.0
#
# Run tests (from host, after server is up):
#   ANTHROPIC_BASE_URL=http://localhost:8000 uv run pytest test/anthropic/tests/ -v

# =============================================================================
# STAGE 1: Base image with all dependencies and cubins (heavy, cached)
# =============================================================================
ARG CUDA_VERSION=12.9.1
# BASE_IMAGE must be declared before any FROM to be used in FROM
ARG BASE_IMAGE=base
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu22.04 AS base

ARG DEEPEP_VERSION=v1.2.1
# Control build parallelism to avoid OOM (default ninja uses all cores = 200GB+ RAM)
# Set to 4 for ~32GB RAM, 8 for ~64GB RAM, etc.
ARG BUILD_PARALLEL=4

ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
# Force ninja/cmake to respect our parallelism limits
ENV CMAKE_BUILD_PARALLEL_LEVEL=${BUILD_PARALLEL}
ENV MAKEFLAGS="-j${BUILD_PARALLEL}"

RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    wget \
    curl \
    git \
    build-essential \
    cmake \
    ninja-build \
    && add-apt-repository ppa:deadsnakes/ppa -y \
    && apt-get update \
    && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    # RDMA/InfiniBand and NUMA libs
    libibverbs-dev \
    librdmacm-dev \
    libnuma-dev \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1 \
    && wget https://bootstrap.pypa.io/get-pip.py \
    && python3 get-pip.py \
    && rm get-pip.py \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

ENV UV_VERSION=0.8.13
RUN curl -LsSf https://astral.sh/uv/${UV_VERSION}/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

# Build DeepEP (without NVSHMEM - only needed for multi-node)
ARG BUILD_PARALLEL
RUN --mount=type=cache,target=/root/.cache/pip \
    git clone --depth=1 --branch ${DEEPEP_VERSION} https://github.com/deepseek-ai/DeepEP.git /tmp/DeepEP \
    && cd /tmp/DeepEP \
    && pip install torch==2.8.0 --extra-index-url https://download.pytorch.org/whl/cu129 \
    && TORCH_CUDA_ARCH_LIST="9.0 10.0 12.0a" \
       MAX_JOBS=${BUILD_PARALLEL} \
       pip install --no-build-isolation . \
    && rm -rf /tmp/DeepEP

WORKDIR /sgl-workspace

# Install SGLang dependencies WITHOUT the code (for layer caching)
# We install from a minimal pyproject.toml that just has dependencies
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,target=/root/.cache/pip \
    pip install sglang[all] \
    --extra-index-url https://download.pytorch.org/whl/cu129 \
    && pip uninstall -y sglang

# Download cubins (the slow 10GB part) - this layer is cached
ARG BUILD_PARALLEL
RUN FLASHINFER_CUBIN_DOWNLOAD_THREADS=${BUILD_PARALLEL} \
    FLASHINFER_LOGGING_LEVEL=warning \
    python -m flashinfer --download-cubin || true

# =============================================================================
# STAGE 2: Dev image with code (lightweight, rebuilds fast)
# =============================================================================
FROM ${BASE_IMAGE} AS dev

WORKDIR /sgl-workspace/sglang

# Copy only the code - this is the only layer that changes frequently
COPY python /sgl-workspace/sglang/python
COPY test /sgl-workspace/sglang/test

# Install sglang in editable mode (fast, just creates .pth file)
RUN --mount=type=cache,target=/root/.cache/uv \
    cd /sgl-workspace/sglang/python && \
    uv pip install --system -e ".[dev]" \
    --extra-index-url https://download.pytorch.org/whl/cu129 \
    --index-strategy unsafe-best-match

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-m", "sglang.launch_server", "--help"]
