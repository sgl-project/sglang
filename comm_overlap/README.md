### TE integration in sglang (v0.1.0)

#### How to run the code

It is recommended to use the pre-built docker image. The docker image is built with TE (v1.14.0.dev0+994f19d) and torch (2.5.1+cu124). You can use the docker image via `docker pull zhuohaol/sglang-te:latest`. Please do not modify the torch version in the docker image.

To launch the sglang server with TE enabled, run:

```bash
docker run -it --shm-size 32g --gpus all -p 30001:30001 --ipc=host --rm zhuohaol/sglang-te:latest

git clone https://github.com/Zhuohao-Li/sglang/tree/zhuohaol-comm-overlap

cd sglang/python

python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0 --enable-te
```

Methods to request the server remains the same.

Key changes:

- In `sglang.srt.models`, we add `llama_te_sgl.py` to support llama models with TE. (`llama_te.py` is a another support version, but it is not recommended to use it now.)
- In `sglang.model_executor.model_runner.py` and `server_args.py`, we add corresponding arguments to enable TE when launching the engine.
- In `comm_overlap.profile`, we provide some scripts to profile performance with torch/te
- In `comm_overlap.benchmark`, we provide the benchmark instructions for the latency of the TE model.
- In `comm_overlap.TE`, it is an example of using TE features (comm_overlap and fp8) to do inference.

#### Benchmark

To run the benchmark, it is recommended to use the script `sglang.benchmark_one_batch`:

native pytorch:
```bash
python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3-8B-Instruct --batch 1 16 64 128 --input-len 256 512 --output-len 32 256 --run-name test_run --tp 4
```

TE:
```bash
python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3-8B-Instruct --batch 1 16 64 128 --input-len 256 512 --output-len 32 256 --run-name test_run --tp 4 --enable-te
```

`llama_latency_figure_visual.py` is a script to visualize the latency of the prefill/decoding latency of both torch/TE model. Please replace the data with the `result.jsonl` generated by the benchmark script. Other `.jsonl` files are generated by older benchmark scripts with `sglang.benchmark_server_latency`.

#### TODO

- [ ] Evaluate TE performance increase with different configs and show the performance gain.
- [ ] Add more models support
- [ ] Add fp8 support

#### Reference

- We use NVIDIA [TE](https://github.com/NVIDIA/TransformerEngine/tree/main)(v1.14.0.dev0+994f19d) APIs to build the model.
